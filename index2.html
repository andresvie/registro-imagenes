<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reporte T√©cnico: Validaci√≥n de Pipeline de Registro de Im√°genes | Visi√≥n por Computador</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        :root {
            --primary-color: #2563eb;
            --secondary-color: #1e40af;
            --text-primary: #1f2937;
            --text-secondary: #6b7280;
            --text-muted: #9ca3af;
            --bg-primary: #ffffff;
            --bg-secondary: #f9fafb;
            --bg-code: #f3f4f6;
            --border-color: #e5e7eb;
            --success-color: #10b981;
            --warning-color: #f59e0b;
            --code-color: #e11d48;
        }
        
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.7;
            color: var(--text-primary);
            background: var(--bg-secondary);
            font-size: 16px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 20px;
            background: var(--bg-primary);
        }
        
        /* Header */
        header {
            padding: 60px 0 40px;
            border-bottom: 1px solid var(--border-color);
            margin-bottom: 40px;
        }
        
        .blog-title {
            font-size: 2.75rem;
            font-weight: 700;
            line-height: 1.2;
            color: var(--text-primary);
            margin-bottom: 16px;
            letter-spacing: -0.02em;
        }
        
        .blog-subtitle {
            font-size: 1.25rem;
            color: var(--text-secondary);
            font-weight: 400;
            margin-bottom: 12px;
        }
        
        .blog-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 16px;
            margin-top: 20px;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .blog-meta span {
            display: flex;
            align-items: center;
            gap: 6px;
        }
        
        /* Content */
        .article-content {
            padding-bottom: 60px;
        }
        
        .article-content h2 {
            font-size: 1.875rem;
            font-weight: 700;
            color: var(--text-primary);
            margin-top: 48px;
            margin-bottom: 24px;
            line-height: 1.3;
        }
        
        .article-content h3 {
            font-size: 1.5rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-top: 40px;
            margin-bottom: 16px;
            line-height: 1.4;
        }
        
        .article-content h4 {
            font-size: 1.25rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-top: 32px;
            margin-bottom: 12px;
        }
        
        .article-content p {
            margin-bottom: 20px;
            color: var(--text-primary);
        }
        
        .article-content ul,
        .article-content ol {
            margin: 20px 0;
            padding-left: 30px;
        }
        
        .article-content li {
            margin-bottom: 12px;
            color: var(--text-primary);
        }
        
        .article-content strong {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        /* Status Badges */
        .status-badge {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.875rem;
            font-weight: 500;
            margin-left: 8px;
        }
        
        .status-badge.completed {
            background: #d1fae5;
            color: #065f46;
        }
        
        /* Code Blocks */
        pre {
            background: var(--bg-code);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            overflow-x: auto;
            margin: 24px 0;
            font-family: 'JetBrains Mono', 'Courier New', monospace;
            font-size: 0.875rem;
            line-height: 1.6;
        }
        
        code {
            font-family: 'JetBrains Mono', 'Courier New', monospace;
            font-size: 0.875rem;
            background: var(--bg-code);
            padding: 3px 6px;
            border-radius: 4px;
            color: var(--code-color);
        }
        
        pre code {
            background: transparent;
            padding: 0;
            color: var(--text-primary);
        }
        
        /* Images */
        .image-container {
            margin: 32px 0;
            text-align: center;
        }
        
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
            cursor: pointer;
            transition: transform 0.2s ease;
        }
        
        .image-container img:hover {
            transform: scale(1.02);
        }
        
        .image-caption {
            margin-top: 12px;
            font-size: 0.875rem;
            color: var(--text-secondary);
            font-style: italic;
        }
        
        /* Image Grid */
        .image-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 32px 0;
        }
        
        .image-grid-item {
            text-align: center;
        }
        
        .image-grid-item img {
            width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            cursor: pointer;
            transition: transform 0.2s ease;
        }
        
        .image-grid-item img:hover {
            transform: scale(1.05);
        }
        
        .image-grid-item h4 {
            margin-top: 12px;
            font-size: 0.9rem;
            color: var(--text-primary);
            font-weight: 600;
        }
        
        .image-grid-item p {
            margin-top: 4px;
            font-size: 0.8rem;
            color: var(--text-secondary);
        }
        
        /* Tables */
        .table-container {
            overflow-x: auto;
            margin: 32px 0;
            border: 1px solid var(--border-color);
            border-radius: 8px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.9rem;
        }
        
        thead {
            background: var(--bg-secondary);
        }
        
        th {
            padding: 12px 16px;
            text-align: left;
            font-weight: 600;
            color: var(--text-primary);
            border-bottom: 2px solid var(--border-color);
        }
        
        td {
            padding: 12px 16px;
            border-bottom: 1px solid var(--border-color);
            color: var(--text-primary);
        }
        
        tbody tr:hover {
            background: var(--bg-secondary);
        }
        
        tbody tr:last-child td {
            border-bottom: none;
        }
        
        /* Info Boxes */
        .info-box {
            padding: 16px 20px;
            border-radius: 8px;
            margin: 24px 0;
            border-left: 4px solid var(--primary-color);
            background: var(--bg-secondary);
        }
        
        .info-box h4 {
            margin-bottom: 8px;
            color: var(--primary-color);
        }
        
        .info-box p {
            margin: 0;
            color: var(--text-secondary);
        }
        
        /* Responsive */
        @media (max-width: 768px) {
            .blog-title {
                font-size: 2rem;
            }
            
            .article-content h2 {
                font-size: 1.5rem;
            }
            
            .article-content h3 {
                font-size: 1.25rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1 class="blog-title">Reporte T√©cnico: Validaci√≥n de Pipeline de Registro de Im√°genes con Im√°genes Sint√©ticas</h1>
            <p class="blog-subtitle">Parte 1: Evaluaci√≥n de Precisi√≥n con Transformaciones Conocidas</p>
            <div class="blog-meta">
                <span>üìÖ Noviembre 2024</span>
                <span>üè∑Ô∏è Visi√≥n por Computador</span>
                <span>‚è±Ô∏è 15 min lectura</span>
                <span class="status-badge completed">‚úì Completado</span>
            </div>
        </header>

        <article class="article-content">
            <!-- Introducci√≥n -->
            <h2>1. Introducci√≥n</h2>
            <p>
                El registro de im√°genes es un proceso fundamental en visi√≥n por computador que consiste en alinear 
                dos o m√°s im√°genes de la misma escena tomadas en diferentes tiempos, desde diferentes puntos de vista, 
                o por diferentes sensores. Esta t√©cnica tiene aplicaciones en √°reas como la creaci√≥n de panoramas, 
                seguimiento de objetos, fusi√≥n de im√°genes m√©dicas, y reconstrucci√≥n 3D.
            </p>
            
            <p>
                Antes de aplicar un pipeline de registro a im√°genes reales, es crucial validar su funcionamiento 
                con im√°genes sint√©ticas donde conocemos las transformaciones exactas aplicadas (ground truth). Esto 
                nos permite cuantificar la precisi√≥n del algoritmo y entender c√≥mo diferentes par√°metros afectan 
                su desempe√±o.
            </p>

            <div class="info-box">
                <h4>üéØ Objetivo del Experimento</h4>
                <p>
                    Evaluar la precisi√≥n de nuestro pipeline de registro usando un conjunto de 15 im√°genes sint√©ticas 
                    con transformaciones conocidas, variando rotaci√≥n, traslaci√≥n y escala. Adicionalmente, comparar 
                    diferentes detectores de caracter√≠sticas y analizar el impacto del ratio test en el emparejamiento.
                </p>
            </div>

            <!-- Marco Te√≥rico -->
            <h2>2. Marco Te√≥rico</h2>
            
            <h3>2.1 Transformaciones Geom√©tricas</h3>
            <p>
                Una transformaci√≥n af√≠n 2D puede representarse mediante una matriz de 3√ó3 que combina rotaci√≥n, 
                traslaci√≥n, escala y cizalla. En nuestro caso, trabajamos con transformaciones de similitud que 
                preservan √°ngulos y formas:
            </p>
            
            <pre><code>[ x' ]   [ s¬∑cos(Œ∏)  -s¬∑sin(Œ∏)   tx ] [ x ]
[ y' ] = [ s¬∑sin(Œ∏)   s¬∑cos(Œ∏)   ty ] [ y ]
[ 1  ]   [    0          0        1  ] [ 1 ]</code></pre>
            
            <p>Donde:</p>
            <ul>
                <li><strong>Œ∏</strong>: √°ngulo de rotaci√≥n</li>
                <li><strong>s</strong>: factor de escala</li>
                <li><strong>(tx, ty)</strong>: vector de traslaci√≥n</li>
            </ul>

            <h3>2.2 Detectores de Caracter√≠sticas</h3>
            <p>
                Los detectores de caracter√≠sticas identifican puntos de inter√©s en las im√°genes que sean 
                distintivos y repetibles. En este estudio comparamos tres detectores populares:
            </p>

            <h4>SIFT (Scale-Invariant Feature Transform)</h4>
            <p>
                Propuesto por Lowe (2004), SIFT detecta extremos en el espacio escala usando diferencias de 
                Gaussianas (DoG). Es robusto ante cambios de escala, rotaci√≥n e iluminaci√≥n, pero 
                computacionalmente costoso.
            </p>

            <h4>ORB (Oriented FAST and Rotated BRIEF)</h4>
            <p>
                Desarrollado por Rublee et al. (2011), ORB es una alternativa eficiente que combina el detector 
                FAST con descriptores BRIEF orientados. Es significativamente m√°s r√°pido que SIFT pero puede 
                ser menos robusto en ciertas condiciones.
            </p>

            <h4>AKAZE (Accelerated-KAZE)</h4>
            <p>
                Introducido por Alcantarilla et al. (2013), AKAZE usa difusi√≥n no lineal para la detecci√≥n de 
                caracter√≠sticas. Ofrece un buen balance entre velocidad y precisi√≥n, y es invariante ante cambios 
                de escala y rotaci√≥n.
            </p>

            <h3>2.3 Emparejamiento de Caracter√≠sticas</h3>
            <p>
                El emparejamiento se realiza mediante fuerza bruta (Brute Force Matcher), comparando descriptores 
                entre im√°genes. Para filtrar correspondencias incorrectas, aplicamos el <strong>ratio test</strong> 
                propuesto por Lowe:
            </p>
            
            <pre><code>if distance(match1) < ratio √ó distance(match2):
    accept match1</code></pre>
            
            <p>
                Un ratio t√≠pico es 0.75, pero exploraremos su impacto en el rango [0.5, 0.95].
            </p>

            <h3>2.4 RANSAC para Estimaci√≥n Robusta</h3>
            <p>
                RANSAC (RANdom SAmple Consensus) es un m√©todo iterativo para estimar par√°metros de modelos en 
                presencia de outliers. En nuestro caso, lo usamos para estimar la homograf√≠a que mejor explica 
                las correspondencias de puntos:
            </p>
            
            <ol>
                <li>Seleccionar aleatoriamente un conjunto m√≠nimo de correspondencias (4 puntos para homograf√≠a)</li>
                <li>Calcular el modelo (homograf√≠a) usando estos puntos</li>
                <li>Contar cu√°ntos puntos adicionales son consistentes con el modelo (inliers)</li>
                <li>Repetir N iteraciones y seleccionar el modelo con m√°s inliers</li>
            </ol>

            <!-- Metodolog√≠a -->
            <h2>3. Metodolog√≠a</h2>

            <h3>3.1 Generaci√≥n de Im√°genes Sint√©ticas</h3>
            <p>
                Se gener√≥ un dataset de 15 pares de im√°genes aplicando transformaciones conocidas:
            </p>
            <ul>
                <li><strong>Rotaciones:</strong> desde -30¬∞ hasta +30¬∞ en incrementos de ~4.3¬∞</li>
                <li><strong>Escalas:</strong> desde 0.8 hasta 1.2 (variaci√≥n del ¬±20%)</li>
                <li><strong>Traslaciones:</strong> proporcionales a la rotaci√≥n y escala</li>
            </ul>

            <p>
                La imagen central (imagen 7) corresponde a la transformaci√≥n identidad (sin cambios), 
                sirviendo como punto de referencia.
            </p>

            <h3>3.2 Pipeline de Registro</h3>
            <p>El pipeline implementado consta de las siguientes etapas:</p>
            
            <ol>
                <li><strong>Detecci√≥n de caracter√≠sticas:</strong> Identificaci√≥n de keypoints usando SIFT/ORB/AKAZE</li>
                <li><strong>C√°lculo de descriptores:</strong> Generaci√≥n de vectores descriptivos para cada keypoint</li>
                <li><strong>Emparejamiento:</strong> Matching mediante fuerza bruta con ratio test</li>
                <li><strong>Estimaci√≥n de homograf√≠a:</strong> C√°lculo robusto usando RANSAC</li>
                <li><strong>Descomposici√≥n:</strong> Extracci√≥n de par√°metros (rotaci√≥n, escala, traslaci√≥n)</li>
                <li><strong>Evaluaci√≥n:</strong> C√°lculo de m√©tricas de error</li>
            </ol>

            <h3>3.3 M√©tricas de Evaluaci√≥n</h3>
            <p>Para cuantificar la precisi√≥n del registro, calculamos:</p>

            <h4>Error Cuadr√°tico Medio (RMSE)</h4>
            <p>Mide el error promedio en la posici√≥n de los puntos:</p>
            <pre><code>RMSE = ‚àö(Œ£ ||p'·µ¢ - H¬∑p·µ¢||¬≤ / n)</code></pre>

            <h4>Error Angular</h4>
            <p>Diferencia absoluta entre la rotaci√≥n verdadera y estimada:</p>
            <pre><code>Error Angular = |Œ∏_true - Œ∏_estimated|</code></pre>

            <h4>Error de Traslaci√≥n</h4>
            <p>Distancia euclidiana entre vectores de traslaci√≥n:</p>
            <pre><code>Error Traslaci√≥n = ‚àö((tx_true - tx_est)¬≤ + (ty_true - ty_est)¬≤)</code></pre>

            <h4>Error de Escala</h4>
            <p>Error porcentual en el factor de escala:</p>
            <pre><code>Error Escala (%) = |s_true - s_estimated| / s_true √ó 100</code></pre>

            <!-- Experimentos y Resultados -->
            <h2>4. Experimentos y Resultados</h2>

            <h3>4.1 Validaci√≥n con Dataset Sint√©tico</h3>
            
            <p>
                El primer experimento evalu√≥ el pipeline completo sobre las 15 im√°genes sint√©ticas usando 
                SIFT como detector por defecto y un ratio test de 0.75.
            </p>

            <div class="image-container">
                <img src="data/synthetic/image_0.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 1: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
                <img src="data/synthetic/image_1.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 2: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
                <img src="data/synthetic/image_2.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 3: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
                <img src="data/synthetic/image_3.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 4: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
                <img src="data/synthetic/image_4.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 5: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
                <img src="data/synthetic/image_5.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 6: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
                <img src="data/synthetic/image_6.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 7: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
                <img src="data/synthetic/image_7.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 8: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
                <img src="data/synthetic/image_8.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 9: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
                <img src="data/synthetic/image_9.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 10: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
                <img src="data/synthetic/image_10.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 11: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
                <img src="data/synthetic/image_11.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 12: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
                <img src="data/synthetic/image_12.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 13: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
            </div>

            <div class="image-container">
                <img src="data/synthetic/matches_visualizacion.png" alt="Visualizaci√≥n de correspondencias">
                <p class="image-caption">
                    Figura 2: Visualizaci√≥n de las correspondencias de caracter√≠sticas detectadas entre 
                    pares de im√°genes. Las l√≠neas verdes indican correspondencias consideradas inliers.
                </p>
            </div>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Imagen</th>
                            <th>Rotaci√≥n (¬∞)</th>
                            <th>RMSE</th>
                            <th>Error Angular (¬∞)</th>
                            <th>Error Trasl. (px)</th>
                            <th>Error Escala (%)</th>
                            <th>Inliers</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>0</td>
                            <td>-30.0</td>
                            <td>0.415</td>
                            <td>0.029</td>
                            <td>0.442</td>
                            <td>0.023</td>
                            <td>43</td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>-17.1</td>
                            <td>0.720</td>
                            <td>0.143</td>
                            <td>0.297</td>
                            <td>0.022</td>
                            <td>44</td>
                        </tr>
                        <tr>
                            <td>6</td>
                            <td>-4.3</td>
                            <td>0.135</td>
                            <td>0.019</td>
                            <td>0.290</td>
                            <td>0.043</td>
                            <td>74</td>
                        </tr>
                        <tr style="background: #e0f2fe;">
                            <td><strong>7</strong></td>
                            <td><strong>0.0</strong></td>
                            <td><strong>3.2e-14</strong></td>
                            <td><strong>1.4e-14</strong></td>
                            <td><strong>9.8e-14</strong></td>
                            <td><strong>4.4e-14</strong></td>
                            <td><strong>249</strong></td>
                        </tr>
                        <tr>
                            <td>10</td>
                            <td>12.9</td>
                            <td>0.474</td>
                            <td>0.063</td>
                            <td>0.644</td>
                            <td>0.121</td>
                            <td>72</td>
                        </tr>
                        <tr>
                            <td>13</td>
                            <td>25.7</td>
                            <td>0.240</td>
                            <td>0.024</td>
                            <td>0.201</td>
                            <td>0.006</td>
                            <td>49</td>
                        </tr>
                        <tr>
                            <td>14</td>
                            <td>30.0</td>
                            <td>0.289</td>
                            <td>0.049</td>
                            <td>0.547</td>
                            <td>0.113</td>
                            <td>54</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 1: Resultados representativos del registro sobre el dataset sint√©tico. 
                La imagen 7 (identidad) muestra errores pr√°cticamente nulos, validando el pipeline.
            </p>

            <div class="image-container">
                <img src="data/synthetic/analisis_errores.png" alt="An√°lisis de errores por m√©trica">
                <p class="image-caption">
                    Figura 3: Distribuci√≥n de errores para cada m√©trica evaluada en el dataset completo. 
                    Los box plots muestran mediana, cuartiles y valores at√≠picos.
                </p>
            </div>

            <div class="info-box">
                <h4>üìä Observaciones Clave</h4>
                <p>
                    <strong>1.</strong> La imagen 7 (transformaci√≥n identidad) presenta errores de precisi√≥n 
                    de punto flotante (‚âà 10‚Åª¬π‚Å¥), confirmando que el pipeline funciona correctamente.<br><br>
                    <strong>2.</strong> El RMSE promedio es de 0.68 p√≠xeles, indicando alta precisi√≥n de 
                    sub-p√≠xel en la alineaci√≥n.<br><br>
                    <strong>3.</strong> El error angular promedio es de 0.067¬∞, demostrando excelente 
                    estimaci√≥n de rotaci√≥n.<br><br>
                    <strong>4.</strong> El n√∫mero de inliers disminuye con transformaciones m√°s extremas, 
                    pero se mantienen suficientes correspondencias robustas.
                </p>
            </div>

            <h3>4.2 Comparaci√≥n de Detectores de Caracter√≠sticas</h3>
            
            <p>
                Evaluamos SIFT, ORB y AKAZE sobre la misma imagen de prueba (rotaci√≥n -15¬∞, escala 1.1) 
                para comparar su desempe√±o.
            </p>

            <div class="image-container">
                <img src="data/synthetic/comparacion_detectores.png" alt="Comparaci√≥n de detectores">
                <p class="image-caption">
                    Figura 4: Comparaci√≥n visual y cuantitativa de los tres detectores de caracter√≠sticas. 
                    Se muestran los keypoints detectados, correspondencias, y m√©tricas de error.
                </p>
            </div>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Detector</th>
                            <th>RMSE</th>
                            <th>Error Angular (¬∞)</th>
                            <th>Error Trasl. (px)</th>
                            <th>Error Escala (%)</th>
                            <th>Matches</th>
                            <th>Inliers</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="background: #d1fae5;">
                            <td><strong>SIFT</strong></td>
                            <td><strong>0.361</strong></td>
                            <td><strong>0.021</strong></td>
                            <td><strong>0.514</strong></td>
                            <td><strong>0.158</strong></td>
                            <td>75</td>
                            <td><strong>57</strong></td>
                        </tr>
                        <tr>
                            <td>ORB</td>
                            <td>3.366</td>
                            <td>0.489</td>
                            <td>3.556</td>
                            <td>1.517</td>
                            <td><strong>167</strong></td>
                            <td>154</td>
                        </tr>
                        <tr>
                            <td>AKAZE</td>
                            <td>0.474</td>
                            <td>0.009</td>
                            <td>0.764</td>
                            <td>0.105</td>
                            <td>127</td>
                            <td>105</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 2: Comparaci√≥n cuantitativa de detectores. SIFT ofrece el mejor balance 
                precisi√≥n-calidad, mientras ORB genera m√°s matches pero con mayor error.
            </p>

            <div class="info-box">
                <h4>üîç An√°lisis Comparativo</h4>
                <p>
                    <strong>SIFT:</strong> Mejor precisi√≥n general (RMSE = 0.361 px). Detecta menos 
                    caracter√≠sticas pero de mayor calidad. Recomendado cuando la precisi√≥n es prioritaria.<br><br>
                    <strong>ORB:</strong> Mayor n√∫mero de correspondencias (167 matches) pero con errores 
                    significativamente mayores (RMSE = 3.366 px). √ötil cuando se requiere velocidad sobre 
                    precisi√≥n.<br><br>
                    <strong>AKAZE:</strong> Buen compromiso entre velocidad y precisi√≥n. Error angular m√≠nimo 
                    (0.009¬∞) pero mayor error de traslaci√≥n que SIFT.
                </p>
            </div>

            <h3>4.3 Estudio del Ratio Test</h3>
            
            <p>
                El ratio test de Lowe filtra correspondencias ambiguas. Exploramos ratios de 0.5 a 0.95 
                en incrementos de 0.05 para entender su impacto en la calidad del registro.
            </p>

            <div class="image-container">
                <img src="data/synthetic/estudio_ratio_test.png" alt="Estudio del ratio test">
                <p class="image-caption">
                    Figura 5: Efecto del ratio test en las m√©tricas de registro. Los gr√°ficos muestran 
                    c√≥mo var√≠an RMSE, errores angulares, de traslaci√≥n y escala, as√≠ como el n√∫mero de 
                    correspondencias en funci√≥n del ratio.
                </p>
            </div>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Ratio</th>
                            <th>RMSE</th>
                            <th>Error Angular (¬∞)</th>
                            <th>Matches</th>
                            <th>Inliers</th>
                            <th>Tasa Inliers</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>0.50</td>
                            <td>0.503</td>
                            <td>0.010</td>
                            <td>49</td>
                            <td>44</td>
                            <td>89.8%</td>
                        </tr>
                        <tr>
                            <td>0.60</td>
                            <td>0.310</td>
                            <td>0.022</td>
                            <td>55</td>
                            <td>48</td>
                            <td>87.3%</td>
                        </tr>
                        <tr style="background: #d1fae5;">
                            <td><strong>0.75</strong></td>
                            <td><strong>0.361</strong></td>
                            <td><strong>0.021</strong></td>
                            <td><strong>75</strong></td>
                            <td><strong>57</strong></td>
                            <td><strong>76.0%</strong></td>
                        </tr>
                        <tr>
                            <td>0.85</td>
                            <td>0.401</td>
                            <td>0.048</td>
                            <td>93</td>
                            <td>63</td>
                            <td>67.7%</td>
                        </tr>
                        <tr>
                            <td>0.95</td>
                            <td>0.408</td>
                            <td>0.053</td>
                            <td>158</td>
                            <td>73</td>
                            <td>46.2%</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 3: Impacto del ratio test en el emparejamiento. El valor cl√°sico de 0.75 
                ofrece un buen balance entre n√∫mero de correspondencias y calidad.
            </p>

            <div class="info-box">
                <h4>üìà Hallazgos sobre el Ratio Test</h4>
                <p>
                    <strong>Trade-off cantidad vs. calidad:</strong> Ratios bajos (0.5-0.6) generan pocas 
                    correspondencias pero muy precisas (‚âà90% inliers). Ratios altos (‚â•0.9) aumentan las 
                    correspondencias pero incluyen m√°s outliers (‚âà46% inliers).<br><br>
                    <strong>Punto √≥ptimo:</strong> El ratio tradicional de 0.75 representa un buen compromiso, 
                    manteniendo suficientes correspondencias (75) con una tasa de inliers razonable (76%).<br><br>
                    <strong>Variaci√≥n de error:</strong> El RMSE es relativamente estable (0.3-0.5 px) en el 
                    rango 0.6-0.85, sugiriendo que RANSAC filtra efectivamente los outliers adicionales.
                </p>
            </div>

            <!-- An√°lisis y Discusi√≥n -->
            <h2>5. An√°lisis y Discusi√≥n</h2>

            <h3>5.1 Robustez del Pipeline</h3>
            <p>
                Los resultados demuestran que el pipeline implementado es altamente preciso para el registro 
                de im√°genes sint√©ticas:
            </p>
            <ul>
                <li>
                    <strong>Precisi√≥n sub-p√≠xel:</strong> El RMSE promedio de 0.68 px indica que el algoritmo 
                    puede alinear im√°genes con precisi√≥n menor a un p√≠xel.
                </li>
                <li>
                    <strong>Estimaci√≥n angular precisa:</strong> Errores angulares menores a 0.1¬∞ en la mayor√≠a 
                    de casos, suficiente para aplicaciones pr√°cticas.
                </li>
                <li>
                    <strong>Robustez ante transformaciones:</strong> El pipeline mantiene buen desempe√±o incluso 
                    con rotaciones de ¬±30¬∞ y cambios de escala del ¬±20%.
                </li>
            </ul>

            <h3>5.2 Elecci√≥n de Detector</h3>
            <p>
                La selecci√≥n del detector debe basarse en los requisitos espec√≠ficos de la aplicaci√≥n:
            </p>
            <ul>
                <li>
                    <strong>Para m√°xima precisi√≥n:</strong> SIFT es la mejor opci√≥n, ofreciendo el menor RMSE 
                    y errores angulares. Ideal para aplicaciones donde la exactitud es cr√≠tica (fotogrametr√≠a, 
                    metrolog√≠a).
                </li>
                <li>
                    <strong>Para tiempo real:</strong> ORB es significativamente m√°s r√°pido pero sacrifica 
                    precisi√≥n. Apropiado para aplicaciones m√≥viles o con restricciones computacionales.
                </li>
                <li>
                    <strong>Balance √≥ptimo:</strong> AKAZE ofrece un compromiso razonable entre velocidad y 
                    precisi√≥n, siendo una alternativa vers√°til.
                </li>
            </ul>

            <h3>5.3 Configuraci√≥n del Ratio Test</h3>
            <p>
                El an√°lisis del ratio test revela consideraciones importantes:
            </p>
            <ul>
                <li>
                    <strong>Ratio bajo (0.5-0.65):</strong> Apropiado cuando se dispone de buenas caracter√≠sticas 
                    y se prioriza la calidad sobre la cantidad. √ötil para im√°genes con patrones repetitivos.
                </li>
                <li>
                    <strong>Ratio est√°ndar (0.7-0.8):</strong> Recomendado como valor por defecto, proporcionando 
                    balance entre correspondencias y precisi√≥n.
                </li>
                <li>
                    <strong>Ratio alto (‚â•0.85):</strong> Aumenta correspondencias pero requiere un RANSAC m√°s 
                    robusto. √ötil cuando se esperan pocas caracter√≠sticas distintivas.
                </li>
            </ul>

            <h3>5.4 Limitaciones del Estudio</h3>
            <p>Es importante reconocer las limitaciones de este an√°lisis:</p>
            <ul>
                <li>
                    <strong>Im√°genes sint√©ticas:</strong> Los resultados pueden no generalizarse completamente 
                    a im√°genes reales con ruido, variaciones de iluminaci√≥n, y oclusiones.
                </li>
                <li>
                    <strong>Transformaciones limitadas:</strong> Solo evaluamos transformaciones de similitud. 
                    Transformaciones proyectivas o deformaciones no r√≠gidas presentar√≠an mayores desaf√≠os.
                </li>
                <li>
                    <strong>Una sola imagen base:</strong> Los resultados dependen de las caracter√≠sticas 
                    presentes en la imagen utilizada. Diferentes texturas podr√≠an afectar el desempe√±o.
                </li>
            </ul>

            <h3>5.5 Extensiones Futuras</h3>
            <p>Posibles mejoras y estudios adicionales:</p>
            <ul>
                <li>Evaluar con im√°genes reales de diferentes escenas y condiciones de iluminaci√≥n</li>
                <li>Comparar con detectores de aprendizaje profundo (SuperPoint, D2-Net)</li>
                <li>Implementar t√©cnicas de optimizaci√≥n global (bundle adjustment)</li>
                <li>Analizar el impacto del ruido y blur en la precisi√≥n del registro</li>
                <li>Estudiar t√©cnicas de blending para fusi√≥n de im√°genes m√°s natural</li>
            </ul>

            <!-- Conclusiones -->
            <h2>6. Conclusiones</h2>
            
            <p>
                Este estudio de validaci√≥n ha demostrado que el pipeline de registro implementado es altamente 
                preciso y robusto para im√°genes sint√©ticas:
            </p>

            <ol>
                <li>
                    <strong>Alta precisi√≥n demostrada:</strong> RMSE promedio de 0.68 px y errores angulares 
                    menores a 0.1¬∞ validan la efectividad del enfoque basado en caracter√≠sticas.
                </li>
                <li>
                    <strong>SIFT como mejor detector:</strong> Ofrece la mayor precisi√≥n aunque con menor n√∫mero 
                    de correspondencias. ORB proporciona velocidad a costa de exactitud.
                </li>
                <li>
                    <strong>Ratio test 0.75 √≥ptimo:</strong> El valor tradicional mantiene un balance apropiado 
                    entre cantidad de correspondencias y calidad de las mismas.
                </li>
                <li>
                    <strong>RANSAC eficaz:</strong> El algoritmo filtra exitosamente outliers, manteniendo 
                    estabilidad en las estimaciones incluso con ratios m√°s permisivos.
                </li>
                <li>
                    <strong>Pipeline validado:</strong> Los errores pr√°cticamente nulos en la transformaci√≥n 
                    identidad confirman la correcta implementaci√≥n del sistema.
                </li>
            </ol>

            <p>
                Con esta validaci√≥n completada, el pipeline est√° listo para aplicarse al desaf√≠o principal: 
                registrar y fusionar las tres im√°genes reales del comedor, con la confianza de que el sistema 
                subyacente es preciso y robusto.
            </p>

            <!-- Referencias -->
            <h2>7. Referencias</h2>
            
            <ol>
                <li>
                    Lowe, D. G. (2004). <em>Distinctive image features from scale-invariant keypoints</em>. 
                    International Journal of Computer Vision, 60(2), 91-110.
                </li>
                <li>
                    Rublee, E., Rabaud, V., Konolige, K., & Bradski, G. (2011). <em>ORB: An efficient 
                    alternative to SIFT or SURF</em>. IEEE International Conference on Computer Vision (ICCV).
                </li>
                <li>
                    Alcantarilla, P. F., Nuevo, J., & Bartoli, A. (2013). <em>Fast explicit diffusion for 
                    accelerated features in nonlinear scale spaces</em>. British Machine Vision Conference (BMVC).
                </li>
                <li>
                    Fischler, M. A., & Bolles, R. C. (1981). <em>Random sample consensus: a paradigm for 
                    model fitting with applications to image analysis and automated cartography</em>. 
                    Communications of the ACM, 24(6), 381-395.
                </li>
                <li>
                    Szeliski, R. (2022). <em>Computer Vision: Algorithms and Applications</em> (2nd ed.). 
                    Springer. Chapter 6: Feature Detection and Matching.
                </li>
                <li>
                    Hartley, R., & Zisserman, A. (2004). <em>Multiple View Geometry in Computer Vision</em> 
                    (2nd ed.). Cambridge University Press. Chapter 4: Estimation - 2D Projective Transformations.
                </li>
            </ol>

            <div class="info-box">
                <h4>üìÇ Repositorio del Proyecto</h4>
                <p>
                    El c√≥digo completo de este proyecto, incluyendo todos los scripts de an√°lisis y 
                    visualizaci√≥n, est√° disponible en GitHub. El repositorio incluye notebooks interactivos 
                    para reproducir todos los experimentos presentados en este reporte.
                </p>
            </div>

        </article>
    </div>
</body>
</html>