<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reporte Técnico: Registro y Fusión de Imágenes | Visión por Computador</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        :root {
            --primary-color: #2563eb;
            --secondary-color: #1e40af;
            --text-primary: #1f2937;
            --text-secondary: #6b7280;
            --text-muted: #9ca3af;
            --bg-primary: #ffffff;
            --bg-secondary: #f9fafb;
            --bg-code: #f3f4f6;
            --border-color: #e5e7eb;
            --success-color: #10b981;
            --warning-color: #f59e0b;
            --code-color: #e11d48;
        }
        
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.7;
            color: var(--text-primary);
            background: var(--bg-secondary);
            font-size: 16px;
        }
        
        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 0 20px;
            background: var(--bg-primary);
        }
        
        /* Header */
        header {
            padding: 60px 0 40px;
            border-bottom: 1px solid var(--border-color);
            margin-bottom: 40px;
        }
        
        .blog-title {
            font-size: 2.5rem;
            font-weight: 700;
            line-height: 1.2;
            color: var(--text-primary);
            margin-bottom: 16px;
            letter-spacing: -0.02em;
        }
        
        .blog-subtitle {
            font-size: 1.1rem;
            color: var(--text-secondary);
            font-weight: 400;
            margin-bottom: 12px;
        }
        
        .blog-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 16px;
            margin-top: 20px;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .blog-meta span {
            display: flex;
            align-items: center;
            gap: 8px;
        }
        
        .blog-meta span i {
            font-size: 0.95rem;
            color: var(--primary-color);
        }
        
        .info-box h4 i {
            margin-right: 8px;
            color: var(--primary-color);
        }
        
        /* Content */
        .article-content {
            padding-bottom: 60px;
        }
        
        .article-content h2 {
            font-size: 1.875rem;
            font-weight: 700;
            color: var(--text-primary);
            margin-top: 48px;
            margin-bottom: 24px;
            line-height: 1.3;
            border-bottom: 2px solid var(--primary-color);
            padding-bottom: 8px;
        }
        
        .article-content h3 {
            font-size: 1.5rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-top: 40px;
            margin-bottom: 16px;
            line-height: 1.4;
        }
        
        .article-content h4 {
            font-size: 1.25rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-top: 32px;
            margin-bottom: 12px;
        }
        
        .article-content p {
            margin-bottom: 20px;
            color: var(--text-primary);
            text-align: justify;
        }
        
        .article-content ul,
        .article-content ol {
            margin: 20px 0;
            padding-left: 30px;
        }
        
        .article-content li {
            margin-bottom: 12px;
            color: var(--text-primary);
        }
        
        .article-content strong {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        /* Code Blocks */
        pre {
            background: var(--bg-code);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            overflow-x: auto;
            margin: 24px 0;
            font-family: 'JetBrains Mono', 'Courier New', monospace;
            font-size: 0.875rem;
            line-height: 1.6;
        }
        
        code {
            font-family: 'JetBrains Mono', 'Courier New', monospace;
            font-size: 0.875rem;
            background: var(--bg-code);
            padding: 3px 6px;
            border-radius: 4px;
            color: var(--code-color);
        }
        
        pre code {
            background: transparent;
            padding: 0;
            color: var(--text-primary);
        }
        
        /* Images */
        .image-container {
            margin: 32px 0;
            text-align: center;
        }
        
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
            cursor: pointer;
            transition: transform 0.2s ease;
        }
        
        .image-container img:hover {
            transform: scale(1.02);
        }
        
        .image-caption {
            margin-top: 12px;
            font-size: 0.875rem;
            color: var(--text-secondary);
            font-style: italic;
        }
        
        /* Image Grid */
        .image-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 32px 0;
        }
        
        .image-grid-item {
            text-align: center;
        }
        
        .image-grid-item img {
            width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            cursor: pointer;
            transition: transform 0.2s ease;
        }
        
        .image-grid-item img:hover {
            transform: scale(1.05);
        }
        
        /* Tables */
        .table-container {
            overflow-x: auto;
            margin: 32px 0;
            border: 1px solid var(--border-color);
            border-radius: 8px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.9rem;
        }
        
        thead {
            background: var(--bg-secondary);
        }
        
        th {
            padding: 12px 16px;
            text-align: left;
            font-weight: 600;
            color: var(--text-primary);
            border-bottom: 2px solid var(--border-color);
        }
        
        td {
            padding: 12px 16px;
            border-bottom: 1px solid var(--border-color);
            color: var(--text-primary);
        }
        
        tbody tr:hover {
            background: var(--bg-secondary);
        }
        
        tbody tr:last-child td {
            border-bottom: none;
        }
        
        /* Info Boxes */
        .info-box {
            padding: 16px 20px;
            border-radius: 8px;
            margin: 24px 0;
            border-left: 4px solid var(--primary-color);
            background: var(--bg-secondary);
        }
        
        .info-box h4 {
            margin-bottom: 8px;
            color: var(--primary-color);
        }
        
        .info-box p {
            margin: 0;
            color: var(--text-secondary);
        }
        
        /* Flow Diagram */
        .flow-diagram {
            background: var(--bg-secondary);
            padding: 24px;
            border-radius: 8px;
            margin: 32px 0;
            border: 1px solid var(--border-color);
        }
        
        .flow-step {
            background: var(--bg-primary);
            padding: 16px;
            margin: 12px 0;
            border-left: 4px solid var(--primary-color);
            border-radius: 4px;
        }
        
        .flow-step h5 {
            margin-bottom: 8px;
            color: var(--primary-color);
            font-weight: 600;
        }
        
        /* Responsive */
        @media (max-width: 768px) {
            .blog-title {
                font-size: 2rem;
            }
            
            .article-content h2 {
                font-size: 1.5rem;
            }
            
            .article-content h3 {
                font-size: 1.25rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1 class="blog-title">Registro y Fusión de Imágenes mediante Detección de Características</h1>
            <p class="blog-subtitle">Validación con Imágenes Sintéticas y Aplicación a Panorámicas Reales</p>
            <div class="blog-meta">
                <span><i class="fas fa-calendar-alt"></i> Semestre 2025-02</span>
                <span><i class="fas fa-tag"></i> Visión por Computador 3009228</span>
                <span><i class="fas fa-university"></i> Universidad Nacional de Colombia - Facultad de Minas</span>
            </div>
        </header>

        <article class="article-content">
            <!-- Introducción -->
            <h2>1. Introducción</h2>
            
            <h3>1.1 Contexto del Problema</h3>
            <p>
                El registro de imágenes es una técnica fundamental en visión por computador que consiste en alinear 
                dos o más imágenes de la misma escena capturadas desde diferentes puntos de vista, en diferentes momentos, 
                o con diferentes sensores. Esta técnica tiene aplicaciones extensas en áreas como la creación de panorámicas, 
                seguimiento de objetos, fusión de imágenes médicas, reconstrucción 3D, y análisis de cambios temporales.
            </p>
            
            <p>
                En el contexto de este proyecto, nos enfocamos en dos objetivos principales: (1) validar la precisión 
                de un pipeline de registro mediante imágenes sintéticas con transformaciones conocidas (ground truth), 
                y (2) aplicar este pipeline validado para crear una panorámica de alta calidad a partir de tres imágenes 
                reales de un comedor, utilizando técnicas avanzadas de blending para lograr transiciones suaves.
            </p>

            <h3>1.2 Motivación</h3>
            <p>
                Antes de aplicar un algoritmo de registro a imágenes reales, es crucial validar su funcionamiento con 
                datos sintéticos donde conocemos las transformaciones exactas aplicadas. Esto nos permite:
            </p>
            <ul>
                <li><strong>Cuantificar la precisión:</strong> Medir errores reales (RMSE, error angular, error de escala) 
                en lugar de aproximaciones visuales.</li>
                <li><strong>Entender limitaciones:</strong> Identificar bajo qué condiciones el algoritmo funciona mejor 
                o peor.</li>
                <li><strong>Optimizar parámetros:</strong> Determinar valores óptimos para detectores, ratio test, y 
                otros hiperparámetros.</li>
                <li><strong>Comparar métodos:</strong> Evaluar objetivamente diferentes detectores de características 
                (SIFT, ORB, AKAZE) y técnicas de blending.</li>
            </ul>

            <div class="info-box">
                <h4><i class="fas fa-bullseye"></i> Objetivos del Proyecto</h4>
                <p>
                    <strong>Parte 1:</strong> Validar el pipeline de registro usando 15 imágenes sintéticas con transformaciones 
                    conocidas, comparando diferentes detectores y analizando el impacto del ratio test.<br><br>
                    <strong>Parte 2:</strong> Crear una panorámica de alta calidad a partir de tres imágenes reales del comedor 
                    usando Image Pyramid Stitching con Laplacian Pyramid Blending, comparando resultados entre detectores.<br><br>
                    <strong>Parte 3:</strong> Realizar calibración y mediciones sobre la panorámica generada para demostrar 
                    aplicaciones prácticas del registro.
                </p>
            </div>

            <!-- Marco Teórico -->
            <h2>2. Marco Teórico</h2>
            
            <h3>2.1 Transformaciones Geométricas</h3>
            <p>
                En visión por computador, las transformaciones geométricas permiten relacionar coordenadas de puntos 
                entre diferentes imágenes. Una transformación afín 2D puede representarse mediante una matriz de 3×3:
            </p>
            
            <pre><code>[ x' ]   [ a  b  tx ] [ x ]
[ y' ] = [ c  d  ty ] [ y ]
[ 1  ]   [ 0  0  1  ] [ 1 ]</code></pre>
            
            <p>
                Para transformaciones de similitud (que preservan ángulos y formas), la matriz se simplifica a:
            </p>
            
            <pre><code>[ x' ]   [ s·cos(θ)  -s·sin(θ)   tx ] [ x ]
[ y' ] = [ s·sin(θ)   s·cos(θ)   ty ] [ y ]
[ 1  ]   [    0          0        1  ] [ 1 ]</code></pre>
            
            <p>Donde:</p>
            <ul>
                <li><strong>θ:</strong> ángulo de rotación</li>
                <li><strong>s:</strong> factor de escala uniforme</li>
                <li><strong>(tx, ty):</strong> vector de traslación</li>
            </ul>

            <p>
                Para panorámicas con cambios de perspectiva más complejos, utilizamos transformaciones proyectivas 
                (homografías), representadas por matrices de 3×3 con 8 grados de libertad:
            </p>
            
            <pre><code>[ x' ]   [ h₁₁  h₁₂  h₁₃ ] [ x ]
[ y' ] = [ h₂₁  h₂₂  h₂₃ ] [ y ]
[ w' ]   [ h₃₁  h₃₂  h₃₃ ] [ 1 ]</code></pre>

            <h3>2.2 Detectores de Características</h3>
            <p>
                Los detectores de características identifican puntos de interés (keypoints) en las imágenes que sean 
                distintivos, repetibles e invariantes ante transformaciones geométricas y fotométricas. En este estudio 
                comparamos tres detectores populares:
            </p>

            <h4>SIFT (Scale-Invariant Feature Transform)</h4>
            <p>
                Propuesto por Lowe (2004), SIFT detecta extremos en el espacio escala usando diferencias de Gaussianas 
                (DoG). Es robusto ante cambios de escala, rotación, iluminación y pequeñas variaciones de punto de vista. 
                Los descriptores SIFT son vectores de 128 dimensiones que codifican información de gradientes locales. 
                Aunque computacionalmente costoso, SIFT ofrece la mayor precisión y robustez.
            </p>

            <h4>ORB (Oriented FAST and Rotated BRIEF)</h4>
            <p>
                Desarrollado por Rublee et al. (2011), ORB es una alternativa eficiente que combina el detector FAST 
                con descriptores BRIEF orientados. Es significativamente más rápido que SIFT (10-100×) pero puede ser 
                menos robusto ante cambios de escala grandes y variaciones de iluminación. Los descriptores ORB son 
                binarios de 256 bits, lo que permite comparaciones rápidas mediante distancia Hamming.
            </p>

            <h4>AKAZE (Accelerated-KAZE)</h4>
            <p>
                Introducido por Alcantarilla et al. (2013), AKAZE usa difusión no lineal para la detección de características 
                en espacios de escala no lineales. Ofrece un buen balance entre velocidad y precisión, siendo invariante 
                ante cambios de escala y rotación. AKAZE es más rápido que SIFT pero mantiene buena robustez.
            </p>

            <h3>2.3 Emparejamiento de Características</h3>
            <p>
                El emparejamiento se realiza mediante fuerza bruta (Brute Force Matcher), comparando descriptores entre 
                imágenes. Para filtrar correspondencias incorrectas, aplicamos el <strong>ratio test</strong> propuesto por Lowe:
            </p>
            
            <pre><code>if distance(match₁) < ratio × distance(match₂):
    accept match₁</code></pre>
            
            <p>
                Un ratio típico es 0.75, pero exploramos su impacto en el rango [0.5, 0.95]. Ratios bajos generan menos 
                correspondencias pero más confiables, mientras que ratios altos aumentan correspondencias pero incluyen 
                más outliers.
            </p>

            <h3>2.4 RANSAC para Estimación Robusta</h3>
            <p>
                RANSAC (RANdom SAmple Consensus), propuesto por Fischler y Bolles (1981), es un método iterativo para 
                estimar parámetros de modelos en presencia de outliers. En nuestro caso, lo usamos para estimar la 
                homografía que mejor explica las correspondencias:
            </p>
            
            <ol>
                <li>Seleccionar aleatoriamente un conjunto mínimo de correspondencias (4 puntos para homografía)</li>
                <li>Calcular el modelo (homografía) usando estos puntos</li>
                <li>Contar cuántos puntos adicionales son consistentes con el modelo (inliers)</li>
                <li>Repetir N iteraciones y seleccionar el modelo con más inliers</li>
            </ol>

            <h3>2.5 Image Pyramid Stitching</h3>
            <p>
                Para crear panorámicas de alta calidad, utilizamos un enfoque basado en pirámides de imágenes:
            </p>
            
            <h4>Gaussian Pyramid</h4>
            <p>
                Una pirámide gaussiana es una representación multi-escala de una imagen, donde cada nivel es una versión 
                suavizada y submuestreada del nivel anterior. Esto permite detectar características a diferentes escalas 
                y mejorar la robustez del matching.
            </p>
            
            <h4>Laplacian Pyramid Blending</h4>
            <p>
                Propuesto por Burt y Adelson (1983), el blending mediante pirámide laplaciana combina imágenes en diferentes 
                bandas de frecuencia. Esto permite transiciones suaves sin halos visibles, especialmente útil cuando hay 
                diferencias de exposición o iluminación entre imágenes. El proceso consiste en:
            </p>
            <ol>
                <li>Construir pirámides laplacianas para ambas imágenes</li>
                <li>Construir pirámides gaussianas para las máscaras de blending</li>
                <li>Mezclar cada nivel de la pirámide por separado</li>
                <li>Reconstruir la imagen final desde la pirámide mezclada</li>
            </ol>

            <!-- Metodología -->
            <h2>3. Metodología</h2>

            <h3>3.1 Descripción del Pipeline Implementado</h3>
            
            <div class="flow-diagram">
                <h4>Pipeline de Registro de Imágenes</h4>
                <div class="flow-step">
                    <h5>1. Detección de Características</h5>
                    <p>Identificación de keypoints usando SIFT/ORB/AKAZE. En el caso de panorámicas, se usa detección 
                    multi-escala mediante pirámide gaussiana.</p>
                </div>
                <div class="flow-step">
                    <h5>2. Cálculo de Descriptores</h5>
                    <p>Generación de vectores descriptivos para cada keypoint detectado.</p>
                </div>
                <div class="flow-step">
                    <h5>3. Emparejamiento</h5>
                    <p>Matching mediante fuerza bruta con ratio test de Lowe para filtrar correspondencias ambiguas.</p>
                </div>
                <div class="flow-step">
                    <h5>4. Estimación de Homografía</h5>
                    <p>Cálculo robusto usando RANSAC para filtrar outliers y estimar la transformación geométrica.</p>
                </div>
                <div class="flow-step">
                    <h5>5. Warping</h5>
                    <p>Aplicación de la transformación para alinear las imágenes en un sistema de coordenadas común.</p>
                </div>
                <div class="flow-step">
                    <h5>6. Blending</h5>
                    <p>Fusión de imágenes usando Laplacian Pyramid Blending para transiciones suaves.</p>
                </div>
            </div>

            <h3>3.2 Justificación de Decisiones Técnicas</h3>
            
            <h4>Selección de Detector: SIFT como Base</h4>
            <p>
                Aunque comparamos SIFT, ORB y AKAZE, SIFT fue seleccionado como detector principal debido a su superior 
                precisión demostrada en la validación con imágenes sintéticas (RMSE = 0.361 px vs 0.474 px de AKAZE y 
                3.366 px de ORB). La mayor robustez de SIFT compensa su mayor costo computacional para aplicaciones donde 
                la precisión es crítica.
            </p>

            <h4>Ratio Test de 0.75</h4>
            <p>
                El análisis del ratio test mostró que valores entre 0.6-0.75 ofrecen el mejor balance entre cantidad y 
                calidad de correspondencias. El valor clásico de 0.75 mantiene suficientes correspondencias (75 matches) 
                con una tasa de inliers razonable (76%), permitiendo que RANSAC filtre efectivamente los outliers adicionales.
            </p>

            <h4>Image Pyramid Stitching</h4>
            <p>
                Se implementó detección multi-escala mediante pirámide gaussiana para mejorar la robustez ante cambios 
                de escala y perspectiva. Esto es especialmente importante en panorámicas donde las imágenes pueden tener 
                diferentes niveles de zoom o ángulos de captura.
            </p>

            <h4>Laplacian Pyramid Blending</h4>
            <p>
                Se eligió blending mediante pirámide laplaciana sobre métodos simples (como feather blending) porque:
            </p>
            <ul>
                <li>Elimina halos visibles en zonas de transición</li>
                <li>Maneja mejor diferencias de exposición e iluminación</li>
                <li>Preserva detalles finos en texturas y bordes</li>
                <li>Produce resultados visualmente superiores en comparación directa</li>
            </ul>

            <h3>3.3 Generación de Dataset Sintético</h3>
            <p>
                Para la validación cuantitativa, se generó un dataset de 15 imágenes sintéticas aplicando transformaciones 
                conocidas a una imagen base con patrones geométricos:
            </p>
            <ul>
                <li><strong>Rotaciones:</strong> desde -30° hasta +30° en incrementos de ~4.3°</li>
                <li><strong>Escalas:</strong> desde 0.8 hasta 1.2 (variación del ±20%)</li>
                <li><strong>Traslaciones:</strong> proporcionales a la rotación y escala</li>
            </ul>
            <p>
                La imagen central (imagen 7) corresponde a la transformación identidad (sin cambios), sirviendo como 
                punto de referencia para validar la precisión del pipeline.
            </p>

            <h3>3.4 Métricas de Evaluación</h3>
            <p>Para cuantificar la precisión del registro, calculamos:</p>

            <h4>Error Cuadrático Medio (RMSE)</h4>
            <p>Mide el error promedio en la posición de los puntos transformados:</p>
            <pre><code>RMSE = √(Σ ||p'ᵢ - H·pᵢ||² / n)</code></pre>

            <h4>Error Angular</h4>
            <p>Diferencia absoluta entre la rotación verdadera y estimada:</p>
            <pre><code>Error Angular = |θ_true - θ_estimated|</code></pre>

            <h4>Error de Traslación</h4>
            <p>Distancia euclidiana entre vectores de traslación:</p>
            <pre><code>Error Traslación = √((tx_true - tx_est)² + (ty_true - ty_est)²)</code></pre>

            <h4>Error de Escala</h4>
            <p>Error porcentual en el factor de escala:</p>
            <pre><code>Error Escala (%) = |s_true - s_estimated| / s_true × 100</code></pre>

            <!-- Experimentos y Resultados -->
            <h2>4. Experimentos y Resultados</h2>

            <h3>4.1 Validación con Imágenes Sintéticas</h3>
            
            <p>
                El primer experimento evaluó el pipeline completo sobre las 15 imágenes sintéticas usando SIFT como 
                detector por defecto y un ratio test de 0.75.
            </p>

            <div class="image-container">
                <img src="results/figures/punto_1/registro_individual.png" alt="Registro individual de imágenes sintéticas">
                <p class="image-caption">
                    Figura 1: Visualización del registro para cada par de imágenes del dataset sintético. 
                    Se muestra la imagen original, la transformada, y la superposición tras el registro.
                </p>
            </div>

            <div class="image-container">
                <img src="results/figures/punto_1/matches_visualizacion.png" alt="Visualización de correspondencias">
                <p class="image-caption">
                    Figura 2: Visualización de las correspondencias de características detectadas entre 
                    pares de imágenes. Las líneas verdes indican correspondencias consideradas inliers por RANSAC.
                </p>
            </div>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Imagen</th>
                            <th>Rotación (°)</th>
                            <th>RMSE (px)</th>
                            <th>Error Angular (°)</th>
                            <th>Error Trasl. (px)</th>
                            <th>Error Escala (%)</th>
                            <th>Inliers</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>0</td>
                            <td>-30.0</td>
                            <td>0.415</td>
                            <td>0.029</td>
                            <td>0.442</td>
                            <td>0.023</td>
                            <td>43</td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>-17.1</td>
                            <td>0.720</td>
                            <td>0.143</td>
                            <td>0.297</td>
                            <td>0.022</td>
                            <td>44</td>
                        </tr>
                        <tr>
                            <td>6</td>
                            <td>-4.3</td>
                            <td>0.135</td>
                            <td>0.019</td>
                            <td>0.290</td>
                            <td>0.043</td>
                            <td>74</td>
                        </tr>
                        <tr style="background: #e0f2fe;">
                            <td><strong>7</strong></td>
                            <td><strong>0.0</strong></td>
                            <td><strong>3.2e-14</strong></td>
                            <td><strong>1.4e-14</strong></td>
                            <td><strong>9.8e-14</strong></td>
                            <td><strong>4.4e-14</strong></td>
                            <td><strong>249</td>
                        </tr>
                        <tr>
                            <td>10</td>
                            <td>12.9</td>
                            <td>0.474</td>
                            <td>0.063</td>
                            <td>0.644</td>
                            <td>0.121</td>
                            <td>72</td>
                        </tr>
                        <tr>
                            <td>13</td>
                            <td>25.7</td>
                            <td>0.240</td>
                            <td>0.024</td>
                            <td>0.201</td>
                            <td>0.006</td>
                            <td>49</td>
                        </tr>
                        <tr>
                            <td>14</td>
                            <td>30.0</td>
                            <td>0.289</td>
                            <td>0.049</td>
                            <td>0.547</td>
                            <td>0.113</td>
                            <td>54</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 1: Resultados representativos del registro sobre el dataset sintético. 
                La imagen 7 (identidad) muestra errores prácticamente nulos, validando el pipeline.
            </p>

            <div class="image-container">
                <img src="results/figures/punto_1/analisis_errores.png" alt="Análisis de errores por métrica">
                <p class="image-caption">
                    Figura 3: Distribución de errores para cada métrica evaluada en el dataset completo. 
                    Los gráficos muestran la relación entre errores y parámetros de transformación.
                </p>
            </div>

            <div class="info-box">
                <h4><i class="fas fa-chart-bar"></i> Observaciones Clave del Dataset Sintético</h4>
                <p>
                    <strong>1. Precisión sub-píxel:</strong> El RMSE promedio es de 0.498 ± 0.312 píxeles, indicando 
                    alta precisión de sub-píxel en la alineación.<br><br>
                    <strong>2. Estimación angular precisa:</strong> El error angular promedio es de 0.061 ± 0.072°, 
                    demostrando excelente estimación de rotación.<br><br>
                    <strong>3. Validación del pipeline:</strong> La imagen 7 (transformación identidad) presenta 
                    errores de precisión de punto flotante (≈ 10⁻¹⁴), confirmando que el pipeline funciona correctamente.<br><br>
                    <strong>4. Robustez:</strong> El número de inliers disminuye con transformaciones más extremas, 
                    pero se mantienen suficientes correspondencias robustas para estimar la homografía.
                </p>
            </div>

            <h3>4.2 Comparación de Detectores de Características</h3>
            
            <p>
                Evaluamos SIFT, ORB y AKAZE sobre la misma imagen de prueba (rotación -15°, escala 1.1) para comparar 
                su desempeño.
            </p>

            <div class="image-container">
                <img src="results/figures/punto_1/comparacion_detectores.png" alt="Comparación de detectores">
                <p class="image-caption">
                    Figura 4: Comparación visual y cuantitativa de los tres detectores de características. 
                    Se muestran los keypoints detectados, correspondencias, y métricas de error.
                </p>
            </div>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Detector</th>
                            <th>RMSE (px)</th>
                            <th>Error Angular (°)</th>
                            <th>Error Trasl. (px)</th>
                            <th>Error Escala (%)</th>
                            <th>Matches</th>
                            <th>Inliers</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="background: #d1fae5;">
                            <td><strong>SIFT</strong></td>
                            <td><strong>0.361</strong></td>
                            <td><strong>0.021</strong></td>
                            <td><strong>0.514</strong></td>
                            <td><strong>0.158</strong></td>
                            <td>75</td>
                            <td><strong>57</strong></td>
                        </tr>
                        <tr>
                            <td>ORB</td>
                            <td>3.366</td>
                            <td>0.489</td>
                            <td>3.556</td>
                            <td>1.517</td>
                            <td><strong>167</strong></td>
                            <td>154</td>
                        </tr>
                        <tr>
                            <td>AKAZE</td>
                            <td>0.474</td>
                            <td>0.009</td>
                            <td>0.764</td>
                            <td>0.105</td>
                            <td>127</td>
                            <td>105</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 2: Comparación cuantitativa de detectores. SIFT ofrece el mejor balance 
                precisión-calidad, mientras ORB genera más matches pero con mayor error.
            </p>

            <div class="info-box">
                <h4><i class="fas fa-search"></i> Análisis Comparativo de Detectores</h4>
                <p>
                    <strong>SIFT:</strong> Mejor precisión general (RMSE = 0.361 px). Detecta menos características 
                    pero de mayor calidad. Recomendado cuando la precisión es prioritaria.<br><br>
                    <strong>ORB:</strong> Mayor número de correspondencias (167 matches) pero con errores 
                    significativamente mayores (RMSE = 3.366 px). Útil cuando se requiere velocidad sobre 
                    precisión, pero no recomendado para aplicaciones que requieren alta precisión.<br><br>
                    <strong>AKAZE:</strong> Buen compromiso entre velocidad y precisión. Error angular mínimo 
                    (0.009°) pero mayor error de traslación que SIFT. Alternativa versátil cuando se necesita 
                    balance entre velocidad y precisión.
                </p>
            </div>

            <h3>4.3 Estudio del Ratio Test</h3>
            
            <p>
                El ratio test de Lowe filtra correspondencias ambiguas. Exploramos ratios de 0.5 a 0.95 en incrementos 
                de 0.05 para entender su impacto en la calidad del registro.
            </p>

            <div class="image-container">
                <img src="results/figures/punto_1/estudio_ratio_test.png" alt="Estudio del ratio test">
                <p class="image-caption">
                    Figura 5: Efecto del ratio test en las métricas de registro. Los gráficos muestran 
                    cómo varían RMSE, errores angulares, de traslación y escala, así como el número de 
                    correspondencias en función del ratio.
                </p>
            </div>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Ratio</th>
                            <th>RMSE (px)</th>
                            <th>Error Angular (°)</th>
                            <th>Matches</th>
                            <th>Inliers</th>
                            <th>Tasa Inliers</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>0.50</td>
                            <td>0.503</td>
                            <td>0.010</td>
                            <td>49</td>
                            <td>44</td>
                            <td>89.8%</td>
                        </tr>
                        <tr>
                            <td>0.60</td>
                            <td>0.310</td>
                            <td>0.022</td>
                            <td>55</td>
                            <td>48</td>
                            <td>87.3%</td>
                        </tr>
                        <tr style="background: #d1fae5;">
                            <td><strong>0.75</strong></td>
                            <td><strong>0.361</strong></td>
                            <td><strong>0.021</strong></td>
                            <td><strong>75</strong></td>
                            <td><strong>57</strong></td>
                            <td><strong>76.0%</strong></td>
                        </tr>
                        <tr>
                            <td>0.85</td>
                            <td>0.401</td>
                            <td>0.048</td>
                            <td>93</td>
                            <td>63</td>
                            <td>67.7%</td>
                        </tr>
                        <tr>
                            <td>0.95</td>
                            <td>0.408</td>
                            <td>0.053</td>
                            <td>158</td>
                            <td>73</td>
                            <td>46.2%</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 3: Impacto del ratio test en el emparejamiento. El valor clásico de 0.75 
                ofrece un buen balance entre número de correspondencias y calidad.
            </p>

            <h3>4.4 Registro y Fusión de Imágenes Reales</h3>
            
            <p>
                Aplicamos el pipeline validado a tres imágenes reales del comedor (IMG01.jpg, IMG02.jpg, IMG03.jpg) 
                usando Image Pyramid Stitching con Laplacian Pyramid Blending.
            </p>

            <div class="image-container">
                <img src="results/panoramic/comparacion_panoramas_detectores_pyramid.jpg" alt="Comparación de panorámicas">
                <p class="image-caption">
                    Figura 6: Panorámicas generadas usando diferentes detectores. De izquierda a derecha: 
                    SIFT, ORB, AKAZE. SIFT produce la mejor alineación y transiciones más suaves.
                </p>
            </div>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Detector</th>
                            <th>Tiempo (s)</th>
                            <th>Tamaño Panorámica</th>
                            <th>Keypoints</th>
                            <th>Matches</th>
                            <th>Inliers</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="background: #d1fae5;">
                            <td><strong>SIFT</strong></td>
                            <td><strong>34.26</strong></td>
                            <td><strong>9395×9612</strong></td>
                            <td><strong>169,810</strong></td>
                            <td><strong>11,926</strong></td>
                            <td><strong>6,537</strong></td>
                        </tr>
                        <tr>
                            <td>ORB</td>
                            <td>11.75</td>
                            <td>9227×9709</td>
                            <td>8,000</td>
                            <td>899</td>
                            <td>592</td>
                        </tr>
                        <tr>
                            <td>AKAZE</td>
                            <td>19.47</td>
                            <td>10251×9379</td>
                            <td>73,528</td>
                            <td>4,173</td>
                            <td>2,250</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 4: Comparación de detectores en la creación de panorámicas. SIFT ofrece la mejor 
                calidad aunque con mayor tiempo de ejecución.
            </p>

            <div class="image-container">
                <img src="results/panoramic/comparacion_metricas_detectores_pyramid.jpg" alt="Comparación de métricas">
                <p class="image-caption">
                    Figura 7: Comparación cuantitativa de métricas entre detectores para la creación de panorámicas.
                </p>
            </div>

            <h3>4.5 Comparación de Técnicas de Blending</h3>
            
            <p>
                Comparamos dos técnicas de blending: Feather blending y Laplacian Pyramid Blending.
            </p>

            <div class="image-grid">
                <div class="image-grid-item">
                    <img src="results/panoramic/pair_SIFT_feather.jpg" alt="Feather blending">
                    <h4>Feather Blending</h4>
                    <p>Transiciones suaves pero con halos visibles en zonas de diferente exposición</p>
                </div>
                <div class="image-grid-item">
                    <img src="results/panoramic/pair_SIFT_laplacian.jpg" alt="Laplacian pyramid blending">
                    <h4>Laplacian Pyramid</h4>
                    <p>Mejor manejo de diferencias de iluminación y preservación de detalles finos</p>
                </div>
            </div>

            <p class="image-caption">
                Figura 8: Comparación de técnicas de blending. Laplacian Pyramid Blending produce 
                resultados visualmente superiores con transiciones más naturales.
            </p>

            <div class="image-container">
                <img src="results/panoramic/trio_SIFT_feather.jpg" alt="Panorámica final con tres imágenes">
                <p class="image-caption">
                    Figura 9: Panorámica final fusionada de las tres imágenes del comedor usando SIFT 
                    y Laplacian Pyramid Blending.
                </p>
            </div>

            <h3>4.6 Calibración y Mediciones</h3>
            
            <p>
                Se realizó calibración usando un objeto de referencia conocido (cuadro de 117 cm) y se aplicaron 
                mediciones sobre la panorámica generada.
            </p>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Tipo</th>
                            <th>Objeto</th>
                            <th>Longitud Medida (px)</th>
                            <th>Longitud Medida (cm)</th>
                            <th>Longitud Real (cm)</th>
                            <th>Error Relativo (%)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="background: #e0f2fe;">
                            <td><strong>Referencia</strong></td>
                            <td><strong>Cuadro</strong></td>
                            <td><strong>2129.69</strong></td>
                            <td><strong>117.0</strong></td>
                            <td><strong>117.0</strong></td>
                            <td><strong>0.0</strong></td>
                        </tr>
                        <tr>
                            <td>Validación</td>
                            <td>Mesa</td>
                            <td>907.85</td>
                            <td>49.87</td>
                            <td>161.1</td>
                            <td>69.04</td>
                        </tr>
                        <tr>
                            <td>Medición</td>
                            <td>Objeto 1</td>
                            <td>565.41</td>
                            <td>31.06</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 5: Resultados de calibración y mediciones sobre la panorámica. La escala calculada 
                fue de 0.055 cm/px. El error en la validación con la mesa indica limitaciones del modelo 
                de homografía ante paralaje en escenas no planas.
            </p>

            <div class="image-container">
                <img src="results/figures/punto_3/mediciones_visualizacion.jpg" alt="Visualización de mediciones">
                <p class="image-caption">
                    Figura 10: Visualización de todas las mediciones realizadas sobre la panorámica generada.
                </p>
            </div>

            <!-- Análisis y Discusión -->
            <h2>5. Análisis y Discusión</h2>

            <h3>5.1 Comparación de Diferentes Métodos Probados</h3>
            
            <h4>Detectores de Características</h4>
            <p>
                La comparación sistemática de SIFT, ORB y AKAZE reveló diferencias significativas:
            </p>
            <ul>
                <li><strong>SIFT:</strong> Ofrece la mayor precisión (RMSE = 0.361 px) y robustez, aunque con mayor 
                costo computacional. Ideal para aplicaciones donde la precisión es crítica.</li>
                <li><strong>ORB:</strong> Significativamente más rápido pero con errores mucho mayores (RMSE = 3.366 px). 
                No recomendado para aplicaciones que requieren alta precisión, aunque puede ser útil para aplicaciones 
                en tiempo real con transformaciones muy pequeñas.</li>
                <li><strong>AKAZE:</strong> Buen compromiso entre velocidad y precisión. Mejor error angular (0.009°) 
                pero mayor error de traslación que SIFT. Alternativa versátil.</li>
            </ul>

            <h4>Técnicas de Blending</h4>
            <p>
                La comparación entre Feather blending y Laplacian Pyramid Blending mostró que:
            </p>
            <ul>
                <li><strong>Feather Blending:</strong> Más rápido computacionalmente pero produce halos visibles en 
                zonas con diferencias de exposición.</li>
                <li><strong>Laplacian Pyramid Blending:</strong> Más costoso pero produce transiciones más naturales 
                y preserva mejor los detalles finos. Recomendado para panorámicas de alta calidad.</li>
            </ul>

            <h3>5.2 Análisis de Errores y Limitaciones</h3>
            
            <h4>Errores en Imágenes Sintéticas</h4>
            <p>
                Los resultados con imágenes sintéticas mostraron excelente precisión:
            </p>
            <ul>
                <li><strong>RMSE promedio:</strong> 0.498 ± 0.312 px (precisión sub-píxel)</li>
                <li><strong>Error angular promedio:</strong> 0.061 ± 0.072° (muy bajo)</li>
                <li><strong>Error de escala promedio:</strong> 0.244 ± 0.304% (excelente)</li>
            </ul>
            <p>
                Sin embargo, se observó que transformaciones más extremas (>25° rotación) pueden aumentar ligeramente 
                los errores, aunque se mantienen dentro de rangos aceptables.
            </p>

            <h4>Limitaciones del Modelo de Homografía</h4>
            <p>
                En la aplicación a imágenes reales, identificamos varias limitaciones:
            </p>
            <ul>
                <li><strong>Paralaje:</strong> El modelo de homografía asume una escena plana. En escenas con múltiples 
                planos de profundidad (como el comedor con objetos cercanos y fondo), se observan duplicaciones y 
                desplazamientos en objetos cercanos.</li>
                <li><strong>Variaciones de Iluminación:</strong> Diferencias de exposición entre imágenes pueden producir 
                transiciones visibles, aunque el Laplacian Pyramid Blending las mitiga significativamente.</li>
                <li><strong>Campo Visual:</strong> Imágenes con ángulos muy diferentes requieren más interpolación en 
                los bordes, aumentando áreas negras del warping.</li>
            </ul>

            <h4>Errores en Mediciones</h4>
            <p>
                El error del 69% en la validación con la mesa (medida estimada: 49.87 cm vs real: 161.1 cm) indica 
                que el modelo de homografía no puede corregir completamente el paralaje. Esto es esperado ya que la 
                mesa está en un plano diferente al cuadro de referencia usado para calibración.
            </p>

            <h3>5.3 Posibles Mejoras</h3>
            
            <p>
                Basado en el análisis de resultados, proponemos las siguientes mejoras:
            </p>

            <h4>Mejoras Técnicas</h4>
            <ul>
                <li><strong>Bundle Adjustment:</strong> Implementar optimización global de todas las homografías 
                simultáneamente para reducir errores acumulativos en panorámicas con múltiples imágenes.</li>
                <li><strong>Seam Finding:</strong> Usar algoritmos de graph-cut o dynamic programming para encontrar 
                costuras óptimas en zonas de transición, eliminando duplicaciones visibles.</li>
                <li><strong>Equalización de Histograma:</strong> Aplicar normalización de histograma por canal antes 
                del registro para uniformar la iluminación entre imágenes.</li>
                <li><strong>Registro Jerárquico:</strong> Para transformaciones muy grandes, aplicar registro en múltiples 
                niveles de escala para mejorar la robustez.</li>
            </ul>

            <h4>Mejoras en Captura</h4>
            <ul>
                <li><strong>Rotación alrededor del centro óptico:</strong> Capturar imágenes rotando solo alrededor 
                del centro óptico de la cámara para minimizar paralaje.</li>
                <li><strong>Mayor solape:</strong> Aumentar el solape entre imágenes consecutivas (60-70%) para mejorar 
                la robustez del matching.</li>
                <li><strong>Exposición consistente:</strong> Usar modo manual de exposición para mantener iluminación 
                consistente entre capturas.</li>
            </ul>

            <h4>Mejoras en Calibración</h4>
            <ul>
                <li><strong>Múltiples objetos de referencia:</strong> Usar varios objetos de referencia en diferentes 
                planos para calibrar mejor el paralaje.</li>
                <li><strong>Modelos 3D:</strong> Para mediciones precisas en escenas no planas, considerar modelos 3D 
                o técnicas de estructura desde movimiento (SfM).</li>
            </ul>

            <!-- Conclusiones -->
            <h2>6. Conclusiones</h2>
            
            <p>
                Este proyecto ha demostrado la efectividad de un pipeline de registro de imágenes basado en detección 
                de características para crear panorámicas de alta calidad. Las principales conclusiones son:
            </p>

            <ol>
                <li>
                    <strong>Validación exitosa:</strong> El pipeline demostró excelente precisión en imágenes sintéticas 
                    con RMSE promedio de 0.498 px y errores angulares menores a 0.1°, validando la correcta implementación 
                    del sistema.
                </li>
                <li>
                    <strong>SIFT como mejor detector:</strong> Entre los tres detectores evaluados, SIFT ofreció la mayor 
                    precisión (RMSE = 0.361 px) y robustez, aunque con mayor costo computacional. ORB resultó inadecuado 
                    para aplicaciones que requieren alta precisión debido a su RMSE significativamente mayor (3.366 px).
                </li>
                <li>
                    <strong>Ratio test óptimo:</strong> El valor tradicional de 0.75 mantiene un balance apropiado 
                    entre cantidad de correspondencias y calidad, aunque valores entre 0.6-0.75 ofrecen resultados 
                    similares.
                </li>
                <li>
                    <strong>Laplacian Pyramid Blending superior:</strong> El blending mediante pirámide laplaciana produjo 
                    resultados visualmente superiores al feather blending, eliminando halos y preservando detalles finos.
                </li>
                <li>
                    <strong>Limitaciones del modelo:</strong> El modelo de homografía tiene limitaciones ante paralaje en 
                    escenas no planas, como se evidenció en los errores de medición. Esto es esperado y requiere técnicas 
                    más avanzadas para corrección completa.
                </li>
                <li>
                    <strong>Aplicabilidad práctica:</strong> El pipeline implementado es adecuado para crear panorámicas 
                    de alta calidad en escenas con características distintivas y solape adecuado entre imágenes.
                </li>
            </ol>

            <p>
                El proyecto demuestra que la validación cuantitativa con imágenes sintéticas es esencial antes de aplicar 
                algoritmos a imágenes reales, permitiendo entender limitaciones y optimizar parámetros de manera objetiva.
            </p>

            <!-- Referencias -->
            <h2>7. Referencias</h2>
            
            <ol>
                <li>
                    Lowe, D. G. (2004). <em>Distinctive image features from scale-invariant keypoints</em>. 
                    International Journal of Computer Vision, 60(2), 91-110.
                </li>
                <li>
                    Rublee, E., Rabaud, V., Konolige, K., & Bradski, G. (2011). <em>ORB: An efficient 
                    alternative to SIFT or SURF</em>. IEEE International Conference on Computer Vision (ICCV).
                </li>
                <li>
                    Alcantarilla, P. F., Nuevo, J., & Bartoli, A. (2013). <em>Fast explicit diffusion for 
                    accelerated features in nonlinear scale spaces</em>. British Machine Vision Conference (BMVC).
                </li>
                <li>
                    Fischler, M. A., & Bolles, R. C. (1981). <em>Random sample consensus: a paradigm for 
                    model fitting with applications to image analysis and automated cartography</em>. 
                    Communications of the ACM, 24(6), 381-395.
                </li>
                <li>
                    Burt, P. J., & Adelson, E. H. (1983). <em>A multiresolution spline with application to 
                    image mosaics</em>. ACM Transactions on Graphics, 2(4), 217-236.
                </li>
                <li>
                    Szeliski, R. (2022). <em>Computer Vision: Algorithms and Applications</em> (2nd ed.). 
                    Springer. Chapter 6: Feature Detection and Matching.
                </li>
                <li>
                    Hartley, R., & Zisserman, A. (2004). <em>Multiple View Geometry in Computer Vision</em> 
                    (2nd ed.). Cambridge University Press. Chapter 4: Estimation - 2D Projective Transformations.
                </li>
                <li>
                    Brown, M., & Lowe, D. G. (2007). <em>Automatic panoramic image stitching using invariant 
                    features</em>. International Journal of Computer Vision, 74(1), 59-73.
                </li>
            </ol>

            <!-- Análisis de Contribución Individual -->
            <h2>8. Análisis de Contribución Individual</h2>
            
            <p>
                Este proyecto fue desarrollado en equipo. A continuación se detalla la contribución de cada integrante:
            </p>

            <h3>8.1 Parte 1: Validación con Imágenes Sintéticas</h3>
            <ul>
                <li><strong>Carlos Andrés Viera Mosquera</strong> (<a href="mailto:cviera@unal.edu.co">cviera@unal.edu.co</a>)</li>
            </ul>
            <p>
                Desarrollo completo de la validación con imágenes sintéticas, incluyendo:
            </p>
            <ul>
                <li>Implementación de funciones para crear imágenes sintéticas con transformaciones conocidas</li>
                <li>Desarrollo de las clases FeatureDetector, FeatureMatcher y RegistrationEvaluator</li>
                <li>Ejecución de experimentos sobre 15 imágenes sintéticas con diferentes transformaciones</li>
                <li>Comparación sistemática de detectores (SIFT, ORB, AKAZE) con análisis cuantitativo</li>
                <li>Estudio del impacto del ratio test en el rango [0.5, 0.95]</li>
                <li>Desarrollo de métricas de error (RMSE, error angular, error de traslación, error de escala)</li>
                <li>Creación del notebook 01_registro_imagenes_proyecto.ipynb con documentación y visualizaciones</li>
            </ul>

            <h3>8.2 Parte 2: Registro de las Imágenes del Comedor</h3>
            <ul>                
                <li><strong>Yenifer Tatiana Guavita Ospino</strong> (<a href="mailto:yguavita@unal.edu.co">yguavita@unal.edu.co</a>)</li>
                <li><strong>Carlos Andrés Viera Mosquera</strong>(<a href="mailto:cviera@unal.edu.co">cviera@unal.edu.co</a>)</li>
            </ul>
            <p>
                Desarrollo conjunto del registro de imágenes reales y creación de panorámicas:
            </p>
            <ul>
                <li>Implementación de la clase Stitcher con detección multi-escala y blending mediante pirámide laplaciana</li>
                <li>Aplicación a imágenes reales: creación de panorámicas a partir de tres imágenes del comedor</li>
                <li>Comparación de técnicas de blending (feather, laplacian pyramid)</li>
                <li>Evaluación de diferentes detectores en el contexto de panorámicas</li>
                <li>Creación del notebook 02_registro_imagenes_comedor.ipynb con análisis y resultados</li>
            </ul>

            <h3>8.3 Parte 3: Calibración y Medición</h3>
            <ul>
                <li><strong>Lina María Montoya Zuluaga</strong> (<a href="mailto:limontoyaz@unal.edu.co">limontoyaz@unal.edu.co</a>)</li>
                <li><strong>Yojan Tamayo Montoya</strong> (<a href="mailto:ytamayom@unal.edu.co">ytamayom@unal.edu.co</a>)</li>
            </ul>
            <p>
                Desarrollo conjunto del sistema de calibración y mediciones:
            </p>
            <ul>
                <li>Implementación del sistema de calibración usando objetos de referencia</li>
                <li>Desarrollo de funciones para medición de objetos sobre la panorámica generada</li>
                <li>Creación del script interactivo punto3.py para mediciones</li>
                <li>Validación de la calibración con objetos conocidos</li>
                <li>Creación del notebook 03_registro_imagenes_calibracion_y_medicion.ipynb</li>
                <li>Generación de visualizaciones con mediciones marcadas</li>
            </ul>

            <h3>8.4 Documentación y Reporte</h3>
            <p>
                La documentación del proyecto, incluyendo este reporte técnico, README.md y visualizaciones, fue desarrollada 
                colaborativamente por todo el equipo.
            </p>

            <div class="info-box">
                <h4><i class="fas fa-folder-open"></i> Repositorio del Proyecto</h4>
                <p>
                    El código completo de este proyecto, incluyendo todos los scripts de análisis, visualización y 
                    notebooks interactivos, está disponible en GitHub. El repositorio incluye documentación detallada 
                    para reproducir todos los experimentos presentados en este reporte.
                </p>
                <p style="margin-top: 12px;">
                    <strong>Repositorio:</strong> 
                    <a href="https://github.com/andresvie/registro-imagenes" target="_blank" style="color: var(--primary-color); text-decoration: none;">
                        <i class="fab fa-github"></i> https://github.com/andresvie/registro-imagenes
                    </a>
                </p>
                <p style="margin-top: 12px;">
                    <strong>Notebooks del proyecto:</strong>
                </p>
                <ul style="margin-top: 8px; padding-left: 30px;">
                    <li>
                        <a href="https://github.com/andresvie/registro-imagenes/blob/main/notebooks/01_registro_imagenes_proyecto.ipynb" target="_blank" style="color: var(--primary-color); text-decoration: none;">
                            <i class="fas fa-book"></i> Parte 1: Validación con Imágenes Sintéticas
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/andresvie/registro-imagenes/blob/main/notebooks/02_registro_imagenes_comedor.ipynb" target="_blank" style="color: var(--primary-color); text-decoration: none;">
                            <i class="fas fa-book"></i> Parte 2: Registro de las Imágenes del Comedor
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/andresvie/registro-imagenes/blob/main/notebooks/03_registro_imagenes_calibracion_y_medicion.ipynb" target="_blank" style="color: var(--primary-color); text-decoration: none;">
                            <i class="fas fa-book"></i> Parte 3: Calibración y Medición
                        </a>
                    </li>
                </ul>
            </div>

        </article>
    </div>
</body>
</html>
