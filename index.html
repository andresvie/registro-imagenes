<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reporte T√©cnico: Registro y Fusi√≥n de Im√°genes | Visi√≥n por Computador</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        :root {
            --primary-color: #2563eb;
            --secondary-color: #1e40af;
            --text-primary: #1f2937;
            --text-secondary: #6b7280;
            --text-muted: #9ca3af;
            --bg-primary: #ffffff;
            --bg-secondary: #f9fafb;
            --bg-code: #f3f4f6;
            --border-color: #e5e7eb;
            --success-color: #10b981;
            --warning-color: #f59e0b;
            --code-color: #e11d48;
        }
        
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.7;
            color: var(--text-primary);
            background: var(--bg-secondary);
            font-size: 16px;
        }
        
        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 0 20px;
            background: var(--bg-primary);
        }
        
        /* Header */
        header {
            padding: 60px 0 40px;
            border-bottom: 1px solid var(--border-color);
            margin-bottom: 40px;
        }
        
        .blog-title {
            font-size: 2.5rem;
            font-weight: 700;
            line-height: 1.2;
            color: var(--text-primary);
            margin-bottom: 16px;
            letter-spacing: -0.02em;
        }
        
        .blog-subtitle {
            font-size: 1.1rem;
            color: var(--text-secondary);
            font-weight: 400;
            margin-bottom: 12px;
        }
        
        .blog-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 16px;
            margin-top: 20px;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .blog-meta span {
            display: flex;
            align-items: center;
            gap: 6px;
        }
        
        /* Content */
        .article-content {
            padding-bottom: 60px;
        }
        
        .article-content h2 {
            font-size: 1.875rem;
            font-weight: 700;
            color: var(--text-primary);
            margin-top: 48px;
            margin-bottom: 24px;
            line-height: 1.3;
            border-bottom: 2px solid var(--primary-color);
            padding-bottom: 8px;
        }
        
        .article-content h3 {
            font-size: 1.5rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-top: 40px;
            margin-bottom: 16px;
            line-height: 1.4;
        }
        
        .article-content h4 {
            font-size: 1.25rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-top: 32px;
            margin-bottom: 12px;
        }
        
        .article-content p {
            margin-bottom: 20px;
            color: var(--text-primary);
            text-align: justify;
        }
        
        .article-content ul,
        .article-content ol {
            margin: 20px 0;
            padding-left: 30px;
        }
        
        .article-content li {
            margin-bottom: 12px;
            color: var(--text-primary);
        }
        
        .article-content strong {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        /* Code Blocks */
        pre {
            background: var(--bg-code);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            overflow-x: auto;
            margin: 24px 0;
            font-family: 'JetBrains Mono', 'Courier New', monospace;
            font-size: 0.875rem;
            line-height: 1.6;
        }
        
        code {
            font-family: 'JetBrains Mono', 'Courier New', monospace;
            font-size: 0.875rem;
            background: var(--bg-code);
            padding: 3px 6px;
            border-radius: 4px;
            color: var(--code-color);
        }
        
        pre code {
            background: transparent;
            padding: 0;
            color: var(--text-primary);
        }
        
        /* Images */
        .image-container {
            margin: 32px 0;
            text-align: center;
        }
        
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
            cursor: pointer;
            transition: transform 0.2s ease;
        }
        
        .image-container img:hover {
            transform: scale(1.02);
        }
        
        .image-caption {
            margin-top: 12px;
            font-size: 0.875rem;
            color: var(--text-secondary);
            font-style: italic;
        }
        
        /* Image Grid */
        .image-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 32px 0;
        }
        
        .image-grid-item {
            text-align: center;
        }
        
        .image-grid-item img {
            width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            cursor: pointer;
            transition: transform 0.2s ease;
        }
        
        .image-grid-item img:hover {
            transform: scale(1.05);
        }
        
        /* Tables */
        .table-container {
            overflow-x: auto;
            margin: 32px 0;
            border: 1px solid var(--border-color);
            border-radius: 8px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.9rem;
        }
        
        thead {
            background: var(--bg-secondary);
        }
        
        th {
            padding: 12px 16px;
            text-align: left;
            font-weight: 600;
            color: var(--text-primary);
            border-bottom: 2px solid var(--border-color);
        }
        
        td {
            padding: 12px 16px;
            border-bottom: 1px solid var(--border-color);
            color: var(--text-primary);
        }
        
        tbody tr:hover {
            background: var(--bg-secondary);
        }
        
        tbody tr:last-child td {
            border-bottom: none;
        }
        
        /* Info Boxes */
        .info-box {
            padding: 16px 20px;
            border-radius: 8px;
            margin: 24px 0;
            border-left: 4px solid var(--primary-color);
            background: var(--bg-secondary);
        }
        
        .info-box h4 {
            margin-bottom: 8px;
            color: var(--primary-color);
        }
        
        .info-box p {
            margin: 0;
            color: var(--text-secondary);
        }
        
        /* Flow Diagram */
        .flow-diagram {
            background: var(--bg-secondary);
            padding: 24px;
            border-radius: 8px;
            margin: 32px 0;
            border: 1px solid var(--border-color);
        }
        
        .flow-step {
            background: var(--bg-primary);
            padding: 16px;
            margin: 12px 0;
            border-left: 4px solid var(--primary-color);
            border-radius: 4px;
        }
        
        .flow-step h5 {
            margin-bottom: 8px;
            color: var(--primary-color);
            font-weight: 600;
        }
        
        /* Responsive */
        @media (max-width: 768px) {
            .blog-title {
                font-size: 2rem;
            }
            
            .article-content h2 {
                font-size: 1.5rem;
            }
            
            .article-content h3 {
                font-size: 1.25rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1 class="blog-title">Registro y Fusi√≥n de Im√°genes mediante Detecci√≥n de Caracter√≠sticas</h1>
            <p class="blog-subtitle">Validaci√≥n con Im√°genes Sint√©ticas y Aplicaci√≥n a Panor√°micas Reales</p>
            <div class="blog-meta">
                <span>üìÖ Semestre 2025-02</span>
                <span>üè∑Ô∏è Visi√≥n por Computador 3009228</span>
                <span>üèõÔ∏è Universidad Nacional de Colombia - Facultad de Minas</span>
            </div>
        </header>

        <article class="article-content">
            <!-- Introducci√≥n -->
            <h2>1. Introducci√≥n</h2>
            
            <h3>1.1 Contexto del Problema</h3>
            <p>
                El registro de im√°genes es una t√©cnica fundamental en visi√≥n por computador que consiste en alinear 
                dos o m√°s im√°genes de la misma escena capturadas desde diferentes puntos de vista, en diferentes momentos, 
                o con diferentes sensores. Esta t√©cnica tiene aplicaciones extensas en √°reas como la creaci√≥n de panor√°micas, 
                seguimiento de objetos, fusi√≥n de im√°genes m√©dicas, reconstrucci√≥n 3D, y an√°lisis de cambios temporales.
            </p>
            
            <p>
                En el contexto de este proyecto, nos enfocamos en dos objetivos principales: (1) validar la precisi√≥n 
                de un pipeline de registro mediante im√°genes sint√©ticas con transformaciones conocidas (ground truth), 
                y (2) aplicar este pipeline validado para crear una panor√°mica de alta calidad a partir de tres im√°genes 
                reales de un comedor, utilizando t√©cnicas avanzadas de blending para lograr transiciones suaves.
            </p>

            <h3>1.2 Motivaci√≥n</h3>
            <p>
                Antes de aplicar un algoritmo de registro a im√°genes reales, es crucial validar su funcionamiento con 
                datos sint√©ticos donde conocemos las transformaciones exactas aplicadas. Esto nos permite:
            </p>
            <ul>
                <li><strong>Cuantificar la precisi√≥n:</strong> Medir errores reales (RMSE, error angular, error de escala) 
                en lugar de aproximaciones visuales.</li>
                <li><strong>Entender limitaciones:</strong> Identificar bajo qu√© condiciones el algoritmo funciona mejor 
                o peor.</li>
                <li><strong>Optimizar par√°metros:</strong> Determinar valores √≥ptimos para detectores, ratio test, y 
                otros hiperpar√°metros.</li>
                <li><strong>Comparar m√©todos:</strong> Evaluar objetivamente diferentes detectores de caracter√≠sticas 
                (SIFT, ORB, AKAZE) y t√©cnicas de blending.</li>
            </ul>

            <div class="info-box">
                <h4>üéØ Objetivos del Proyecto</h4>
                <p>
                    <strong>Parte 1:</strong> Validar el pipeline de registro usando 15 im√°genes sint√©ticas con transformaciones 
                    conocidas, comparando diferentes detectores y analizando el impacto del ratio test.<br><br>
                    <strong>Parte 2:</strong> Crear una panor√°mica de alta calidad a partir de tres im√°genes reales del comedor 
                    usando Image Pyramid Stitching con Laplacian Pyramid Blending, comparando resultados entre detectores.<br><br>
                    <strong>Parte 3:</strong> Realizar calibraci√≥n y mediciones sobre la panor√°mica generada para demostrar 
                    aplicaciones pr√°cticas del registro.
                </p>
            </div>

            <!-- Marco Te√≥rico -->
            <h2>2. Marco Te√≥rico</h2>
            
            <h3>2.1 Transformaciones Geom√©tricas</h3>
            <p>
                En visi√≥n por computador, las transformaciones geom√©tricas permiten relacionar coordenadas de puntos 
                entre diferentes im√°genes. Una transformaci√≥n af√≠n 2D puede representarse mediante una matriz de 3√ó3:
            </p>
            
            <pre><code>[ x' ]   [ a  b  tx ] [ x ]
[ y' ] = [ c  d  ty ] [ y ]
[ 1  ]   [ 0  0  1  ] [ 1 ]</code></pre>
            
            <p>
                Para transformaciones de similitud (que preservan √°ngulos y formas), la matriz se simplifica a:
            </p>
            
            <pre><code>[ x' ]   [ s¬∑cos(Œ∏)  -s¬∑sin(Œ∏)   tx ] [ x ]
[ y' ] = [ s¬∑sin(Œ∏)   s¬∑cos(Œ∏)   ty ] [ y ]
[ 1  ]   [    0          0        1  ] [ 1 ]</code></pre>
            
            <p>Donde:</p>
            <ul>
                <li><strong>Œ∏:</strong> √°ngulo de rotaci√≥n</li>
                <li><strong>s:</strong> factor de escala uniforme</li>
                <li><strong>(tx, ty):</strong> vector de traslaci√≥n</li>
            </ul>

            <p>
                Para panor√°micas con cambios de perspectiva m√°s complejos, utilizamos transformaciones proyectivas 
                (homograf√≠as), representadas por matrices de 3√ó3 con 8 grados de libertad:
            </p>
            
            <pre><code>[ x' ]   [ h‚ÇÅ‚ÇÅ  h‚ÇÅ‚ÇÇ  h‚ÇÅ‚ÇÉ ] [ x ]
[ y' ] = [ h‚ÇÇ‚ÇÅ  h‚ÇÇ‚ÇÇ  h‚ÇÇ‚ÇÉ ] [ y ]
[ w' ]   [ h‚ÇÉ‚ÇÅ  h‚ÇÉ‚ÇÇ  h‚ÇÉ‚ÇÉ ] [ 1 ]</code></pre>

            <h3>2.2 Detectores de Caracter√≠sticas</h3>
            <p>
                Los detectores de caracter√≠sticas identifican puntos de inter√©s (keypoints) en las im√°genes que sean 
                distintivos, repetibles e invariantes ante transformaciones geom√©tricas y fotom√©tricas. En este estudio 
                comparamos tres detectores populares:
            </p>

            <h4>SIFT (Scale-Invariant Feature Transform)</h4>
            <p>
                Propuesto por Lowe (2004), SIFT detecta extremos en el espacio escala usando diferencias de Gaussianas 
                (DoG). Es robusto ante cambios de escala, rotaci√≥n, iluminaci√≥n y peque√±as variaciones de punto de vista. 
                Los descriptores SIFT son vectores de 128 dimensiones que codifican informaci√≥n de gradientes locales. 
                Aunque computacionalmente costoso, SIFT ofrece la mayor precisi√≥n y robustez.
            </p>

            <h4>ORB (Oriented FAST and Rotated BRIEF)</h4>
            <p>
                Desarrollado por Rublee et al. (2011), ORB es una alternativa eficiente que combina el detector FAST 
                con descriptores BRIEF orientados. Es significativamente m√°s r√°pido que SIFT (10-100√ó) pero puede ser 
                menos robusto ante cambios de escala grandes y variaciones de iluminaci√≥n. Los descriptores ORB son 
                binarios de 256 bits, lo que permite comparaciones r√°pidas mediante distancia Hamming.
            </p>

            <h4>AKAZE (Accelerated-KAZE)</h4>
            <p>
                Introducido por Alcantarilla et al. (2013), AKAZE usa difusi√≥n no lineal para la detecci√≥n de caracter√≠sticas 
                en espacios de escala no lineales. Ofrece un buen balance entre velocidad y precisi√≥n, siendo invariante 
                ante cambios de escala y rotaci√≥n. AKAZE es m√°s r√°pido que SIFT pero mantiene buena robustez.
            </p>

            <h3>2.3 Emparejamiento de Caracter√≠sticas</h3>
            <p>
                El emparejamiento se realiza mediante fuerza bruta (Brute Force Matcher), comparando descriptores entre 
                im√°genes. Para filtrar correspondencias incorrectas, aplicamos el <strong>ratio test</strong> propuesto por Lowe:
            </p>
            
            <pre><code>if distance(match‚ÇÅ) < ratio √ó distance(match‚ÇÇ):
    accept match‚ÇÅ</code></pre>
            
            <p>
                Un ratio t√≠pico es 0.75, pero exploramos su impacto en el rango [0.5, 0.95]. Ratios bajos generan menos 
                correspondencias pero m√°s confiables, mientras que ratios altos aumentan correspondencias pero incluyen 
                m√°s outliers.
            </p>

            <h3>2.4 RANSAC para Estimaci√≥n Robusta</h3>
            <p>
                RANSAC (RANdom SAmple Consensus), propuesto por Fischler y Bolles (1981), es un m√©todo iterativo para 
                estimar par√°metros de modelos en presencia de outliers. En nuestro caso, lo usamos para estimar la 
                homograf√≠a que mejor explica las correspondencias:
            </p>
            
            <ol>
                <li>Seleccionar aleatoriamente un conjunto m√≠nimo de correspondencias (4 puntos para homograf√≠a)</li>
                <li>Calcular el modelo (homograf√≠a) usando estos puntos</li>
                <li>Contar cu√°ntos puntos adicionales son consistentes con el modelo (inliers)</li>
                <li>Repetir N iteraciones y seleccionar el modelo con m√°s inliers</li>
            </ol>

            <h3>2.5 Image Pyramid Stitching</h3>
            <p>
                Para crear panor√°micas de alta calidad, utilizamos un enfoque basado en pir√°mides de im√°genes:
            </p>
            
            <h4>Gaussian Pyramid</h4>
            <p>
                Una pir√°mide gaussiana es una representaci√≥n multi-escala de una imagen, donde cada nivel es una versi√≥n 
                suavizada y submuestreada del nivel anterior. Esto permite detectar caracter√≠sticas a diferentes escalas 
                y mejorar la robustez del matching.
            </p>
            
            <h4>Laplacian Pyramid Blending</h4>
            <p>
                Propuesto por Burt y Adelson (1983), el blending mediante pir√°mide laplaciana combina im√°genes en diferentes 
                bandas de frecuencia. Esto permite transiciones suaves sin halos visibles, especialmente √∫til cuando hay 
                diferencias de exposici√≥n o iluminaci√≥n entre im√°genes. El proceso consiste en:
            </p>
            <ol>
                <li>Construir pir√°mides laplacianas para ambas im√°genes</li>
                <li>Construir pir√°mides gaussianas para las m√°scaras de blending</li>
                <li>Mezclar cada nivel de la pir√°mide por separado</li>
                <li>Reconstruir la imagen final desde la pir√°mide mezclada</li>
            </ol>

            <!-- Metodolog√≠a -->
            <h2>3. Metodolog√≠a</h2>

            <h3>3.1 Descripci√≥n del Pipeline Implementado</h3>
            
            <div class="flow-diagram">
                <h4>Pipeline de Registro de Im√°genes</h4>
                <div class="flow-step">
                    <h5>1. Detecci√≥n de Caracter√≠sticas</h5>
                    <p>Identificaci√≥n de keypoints usando SIFT/ORB/AKAZE. En el caso de panor√°micas, se usa detecci√≥n 
                    multi-escala mediante pir√°mide gaussiana.</p>
                </div>
                <div class="flow-step">
                    <h5>2. C√°lculo de Descriptores</h5>
                    <p>Generaci√≥n de vectores descriptivos para cada keypoint detectado.</p>
                </div>
                <div class="flow-step">
                    <h5>3. Emparejamiento</h5>
                    <p>Matching mediante fuerza bruta con ratio test de Lowe para filtrar correspondencias ambiguas.</p>
                </div>
                <div class="flow-step">
                    <h5>4. Estimaci√≥n de Homograf√≠a</h5>
                    <p>C√°lculo robusto usando RANSAC para filtrar outliers y estimar la transformaci√≥n geom√©trica.</p>
                </div>
                <div class="flow-step">
                    <h5>5. Warping</h5>
                    <p>Aplicaci√≥n de la transformaci√≥n para alinear las im√°genes en un sistema de coordenadas com√∫n.</p>
                </div>
                <div class="flow-step">
                    <h5>6. Blending</h5>
                    <p>Fusi√≥n de im√°genes usando Laplacian Pyramid Blending para transiciones suaves.</p>
                </div>
            </div>

            <h3>3.2 Justificaci√≥n de Decisiones T√©cnicas</h3>
            
            <h4>Selecci√≥n de Detector: SIFT como Base</h4>
            <p>
                Aunque comparamos SIFT, ORB y AKAZE, SIFT fue seleccionado como detector principal debido a su superior 
                precisi√≥n demostrada en la validaci√≥n con im√°genes sint√©ticas (RMSE = 0.361 px vs 0.474 px de AKAZE y 
                3.366 px de ORB). La mayor robustez de SIFT compensa su mayor costo computacional para aplicaciones donde 
                la precisi√≥n es cr√≠tica.
            </p>

            <h4>Ratio Test de 0.75</h4>
            <p>
                El an√°lisis del ratio test mostr√≥ que valores entre 0.6-0.75 ofrecen el mejor balance entre cantidad y 
                calidad de correspondencias. El valor cl√°sico de 0.75 mantiene suficientes correspondencias (75 matches) 
                con una tasa de inliers razonable (76%), permitiendo que RANSAC filtre efectivamente los outliers adicionales.
            </p>

            <h4>Image Pyramid Stitching</h4>
            <p>
                Se implement√≥ detecci√≥n multi-escala mediante pir√°mide gaussiana para mejorar la robustez ante cambios 
                de escala y perspectiva. Esto es especialmente importante en panor√°micas donde las im√°genes pueden tener 
                diferentes niveles de zoom o √°ngulos de captura.
            </p>

            <h4>Laplacian Pyramid Blending</h4>
            <p>
                Se eligi√≥ blending mediante pir√°mide laplaciana sobre m√©todos simples (como feather blending) porque:
            </p>
            <ul>
                <li>Elimina halos visibles en zonas de transici√≥n</li>
                <li>Maneja mejor diferencias de exposici√≥n e iluminaci√≥n</li>
                <li>Preserva detalles finos en texturas y bordes</li>
                <li>Produce resultados visualmente superiores en comparaci√≥n directa</li>
            </ul>

            <h3>3.3 Generaci√≥n de Dataset Sint√©tico</h3>
            <p>
                Para la validaci√≥n cuantitativa, se gener√≥ un dataset de 15 im√°genes sint√©ticas aplicando transformaciones 
                conocidas a una imagen base con patrones geom√©tricos:
            </p>
            <ul>
                <li><strong>Rotaciones:</strong> desde -30¬∞ hasta +30¬∞ en incrementos de ~4.3¬∞</li>
                <li><strong>Escalas:</strong> desde 0.8 hasta 1.2 (variaci√≥n del ¬±20%)</li>
                <li><strong>Traslaciones:</strong> proporcionales a la rotaci√≥n y escala</li>
            </ul>
            <p>
                La imagen central (imagen 7) corresponde a la transformaci√≥n identidad (sin cambios), sirviendo como 
                punto de referencia para validar la precisi√≥n del pipeline.
            </p>

            <h3>3.4 M√©tricas de Evaluaci√≥n</h3>
            <p>Para cuantificar la precisi√≥n del registro, calculamos:</p>

            <h4>Error Cuadr√°tico Medio (RMSE)</h4>
            <p>Mide el error promedio en la posici√≥n de los puntos transformados:</p>
            <pre><code>RMSE = ‚àö(Œ£ ||p'·µ¢ - H¬∑p·µ¢||¬≤ / n)</code></pre>

            <h4>Error Angular</h4>
            <p>Diferencia absoluta entre la rotaci√≥n verdadera y estimada:</p>
            <pre><code>Error Angular = |Œ∏_true - Œ∏_estimated|</code></pre>

            <h4>Error de Traslaci√≥n</h4>
            <p>Distancia euclidiana entre vectores de traslaci√≥n:</p>
            <pre><code>Error Traslaci√≥n = ‚àö((tx_true - tx_est)¬≤ + (ty_true - ty_est)¬≤)</code></pre>

            <h4>Error de Escala</h4>
            <p>Error porcentual en el factor de escala:</p>
            <pre><code>Error Escala (%) = |s_true - s_estimated| / s_true √ó 100</code></pre>

            <!-- Experimentos y Resultados -->
            <h2>4. Experimentos y Resultados</h2>

            <h3>4.1 Validaci√≥n con Im√°genes Sint√©ticas</h3>
            
            <p>
                El primer experimento evalu√≥ el pipeline completo sobre las 15 im√°genes sint√©ticas usando SIFT como 
                detector por defecto y un ratio test de 0.75.
            </p>

            <div class="image-container">
                <img src="results/figures/punto_1/registro_individual.png" alt="Registro individual de im√°genes sint√©ticas">
                <p class="image-caption">
                    Figura 1: Visualizaci√≥n del registro para cada par de im√°genes del dataset sint√©tico. 
                    Se muestra la imagen original, la transformada, y la superposici√≥n tras el registro.
                </p>
            </div>

            <div class="image-container">
                <img src="results/figures/punto_1/matches_visualizacion.png" alt="Visualizaci√≥n de correspondencias">
                <p class="image-caption">
                    Figura 2: Visualizaci√≥n de las correspondencias de caracter√≠sticas detectadas entre 
                    pares de im√°genes. Las l√≠neas verdes indican correspondencias consideradas inliers por RANSAC.
                </p>
            </div>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Imagen</th>
                            <th>Rotaci√≥n (¬∞)</th>
                            <th>RMSE (px)</th>
                            <th>Error Angular (¬∞)</th>
                            <th>Error Trasl. (px)</th>
                            <th>Error Escala (%)</th>
                            <th>Inliers</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>0</td>
                            <td>-30.0</td>
                            <td>0.415</td>
                            <td>0.029</td>
                            <td>0.442</td>
                            <td>0.023</td>
                            <td>43</td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>-17.1</td>
                            <td>0.720</td>
                            <td>0.143</td>
                            <td>0.297</td>
                            <td>0.022</td>
                            <td>44</td>
                        </tr>
                        <tr>
                            <td>6</td>
                            <td>-4.3</td>
                            <td>0.135</td>
                            <td>0.019</td>
                            <td>0.290</td>
                            <td>0.043</td>
                            <td>74</td>
                        </tr>
                        <tr style="background: #e0f2fe;">
                            <td><strong>7</strong></td>
                            <td><strong>0.0</strong></td>
                            <td><strong>3.2e-14</strong></td>
                            <td><strong>1.4e-14</strong></td>
                            <td><strong>9.8e-14</strong></td>
                            <td><strong>4.4e-14</strong></td>
                            <td><strong>249</td>
                        </tr>
                        <tr>
                            <td>10</td>
                            <td>12.9</td>
                            <td>0.474</td>
                            <td>0.063</td>
                            <td>0.644</td>
                            <td>0.121</td>
                            <td>72</td>
                        </tr>
                        <tr>
                            <td>13</td>
                            <td>25.7</td>
                            <td>0.240</td>
                            <td>0.024</td>
                            <td>0.201</td>
                            <td>0.006</td>
                            <td>49</td>
                        </tr>
                        <tr>
                            <td>14</td>
                            <td>30.0</td>
                            <td>0.289</td>
                            <td>0.049</td>
                            <td>0.547</td>
                            <td>0.113</td>
                            <td>54</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 1: Resultados representativos del registro sobre el dataset sint√©tico. 
                La imagen 7 (identidad) muestra errores pr√°cticamente nulos, validando el pipeline.
            </p>

            <div class="image-container">
                <img src="results/figures/punto_1/analisis_errores.png" alt="An√°lisis de errores por m√©trica">
                <p class="image-caption">
                    Figura 3: Distribuci√≥n de errores para cada m√©trica evaluada en el dataset completo. 
                    Los gr√°ficos muestran la relaci√≥n entre errores y par√°metros de transformaci√≥n.
                </p>
            </div>

            <div class="info-box">
                <h4>üìä Observaciones Clave del Dataset Sint√©tico</h4>
                <p>
                    <strong>1. Precisi√≥n sub-p√≠xel:</strong> El RMSE promedio es de 0.498 ¬± 0.312 p√≠xeles, indicando 
                    alta precisi√≥n de sub-p√≠xel en la alineaci√≥n.<br><br>
                    <strong>2. Estimaci√≥n angular precisa:</strong> El error angular promedio es de 0.061 ¬± 0.072¬∞, 
                    demostrando excelente estimaci√≥n de rotaci√≥n.<br><br>
                    <strong>3. Validaci√≥n del pipeline:</strong> La imagen 7 (transformaci√≥n identidad) presenta 
                    errores de precisi√≥n de punto flotante (‚âà 10‚Åª¬π‚Å¥), confirmando que el pipeline funciona correctamente.<br><br>
                    <strong>4. Robustez:</strong> El n√∫mero de inliers disminuye con transformaciones m√°s extremas, 
                    pero se mantienen suficientes correspondencias robustas para estimar la homograf√≠a.
                </p>
            </div>

            <h3>4.2 Comparaci√≥n de Detectores de Caracter√≠sticas</h3>
            
            <p>
                Evaluamos SIFT, ORB y AKAZE sobre la misma imagen de prueba (rotaci√≥n -15¬∞, escala 1.1) para comparar 
                su desempe√±o.
            </p>

            <div class="image-container">
                <img src="results/figures/punto_1/comparacion_detectores.png" alt="Comparaci√≥n de detectores">
                <p class="image-caption">
                    Figura 4: Comparaci√≥n visual y cuantitativa de los tres detectores de caracter√≠sticas. 
                    Se muestran los keypoints detectados, correspondencias, y m√©tricas de error.
                </p>
            </div>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Detector</th>
                            <th>RMSE (px)</th>
                            <th>Error Angular (¬∞)</th>
                            <th>Error Trasl. (px)</th>
                            <th>Error Escala (%)</th>
                            <th>Matches</th>
                            <th>Inliers</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="background: #d1fae5;">
                            <td><strong>SIFT</strong></td>
                            <td><strong>0.361</strong></td>
                            <td><strong>0.021</strong></td>
                            <td><strong>0.514</strong></td>
                            <td><strong>0.158</strong></td>
                            <td>75</td>
                            <td><strong>57</strong></td>
                        </tr>
                        <tr>
                            <td>ORB</td>
                            <td>3.366</td>
                            <td>0.489</td>
                            <td>3.556</td>
                            <td>1.517</td>
                            <td><strong>167</strong></td>
                            <td>154</td>
                        </tr>
                        <tr>
                            <td>AKAZE</td>
                            <td>0.474</td>
                            <td>0.009</td>
                            <td>0.764</td>
                            <td>0.105</td>
                            <td>127</td>
                            <td>105</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 2: Comparaci√≥n cuantitativa de detectores. SIFT ofrece el mejor balance 
                precisi√≥n-calidad, mientras ORB genera m√°s matches pero con mayor error.
            </p>

            <div class="info-box">
                <h4>üîç An√°lisis Comparativo de Detectores</h4>
                <p>
                    <strong>SIFT:</strong> Mejor precisi√≥n general (RMSE = 0.361 px). Detecta menos caracter√≠sticas 
                    pero de mayor calidad. Recomendado cuando la precisi√≥n es prioritaria.<br><br>
                    <strong>ORB:</strong> Mayor n√∫mero de correspondencias (167 matches) pero con errores 
                    significativamente mayores (RMSE = 3.366 px). √ötil cuando se requiere velocidad sobre 
                    precisi√≥n, pero no recomendado para aplicaciones que requieren alta precisi√≥n.<br><br>
                    <strong>AKAZE:</strong> Buen compromiso entre velocidad y precisi√≥n. Error angular m√≠nimo 
                    (0.009¬∞) pero mayor error de traslaci√≥n que SIFT. Alternativa vers√°til cuando se necesita 
                    balance entre velocidad y precisi√≥n.
                </p>
            </div>

            <h3>4.3 Estudio del Ratio Test</h3>
            
            <p>
                El ratio test de Lowe filtra correspondencias ambiguas. Exploramos ratios de 0.5 a 0.95 en incrementos 
                de 0.05 para entender su impacto en la calidad del registro.
            </p>

            <div class="image-container">
                <img src="results/figures/punto_1/estudio_ratio_test.png" alt="Estudio del ratio test">
                <p class="image-caption">
                    Figura 5: Efecto del ratio test en las m√©tricas de registro. Los gr√°ficos muestran 
                    c√≥mo var√≠an RMSE, errores angulares, de traslaci√≥n y escala, as√≠ como el n√∫mero de 
                    correspondencias en funci√≥n del ratio.
                </p>
            </div>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Ratio</th>
                            <th>RMSE (px)</th>
                            <th>Error Angular (¬∞)</th>
                            <th>Matches</th>
                            <th>Inliers</th>
                            <th>Tasa Inliers</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>0.50</td>
                            <td>0.503</td>
                            <td>0.010</td>
                            <td>49</td>
                            <td>44</td>
                            <td>89.8%</td>
                        </tr>
                        <tr>
                            <td>0.60</td>
                            <td>0.310</td>
                            <td>0.022</td>
                            <td>55</td>
                            <td>48</td>
                            <td>87.3%</td>
                        </tr>
                        <tr style="background: #d1fae5;">
                            <td><strong>0.75</strong></td>
                            <td><strong>0.361</strong></td>
                            <td><strong>0.021</strong></td>
                            <td><strong>75</strong></td>
                            <td><strong>57</strong></td>
                            <td><strong>76.0%</strong></td>
                        </tr>
                        <tr>
                            <td>0.85</td>
                            <td>0.401</td>
                            <td>0.048</td>
                            <td>93</td>
                            <td>63</td>
                            <td>67.7%</td>
                        </tr>
                        <tr>
                            <td>0.95</td>
                            <td>0.408</td>
                            <td>0.053</td>
                            <td>158</td>
                            <td>73</td>
                            <td>46.2%</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 3: Impacto del ratio test en el emparejamiento. El valor cl√°sico de 0.75 
                ofrece un buen balance entre n√∫mero de correspondencias y calidad.
            </p>

            <h3>4.4 Registro y Fusi√≥n de Im√°genes Reales</h3>
            
            <p>
                Aplicamos el pipeline validado a tres im√°genes reales del comedor (IMG01.jpg, IMG02.jpg, IMG03.jpg) 
                usando Image Pyramid Stitching con Laplacian Pyramid Blending.
            </p>

            <div class="image-container">
                <img src="results/panoramic/comparacion_panoramas_detectores_pyramid.jpg" alt="Comparaci√≥n de panor√°micas">
                <p class="image-caption">
                    Figura 6: Panor√°micas generadas usando diferentes detectores. De izquierda a derecha: 
                    SIFT, ORB, AKAZE. SIFT produce la mejor alineaci√≥n y transiciones m√°s suaves.
                </p>
            </div>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Detector</th>
                            <th>Tiempo (s)</th>
                            <th>Tama√±o Panor√°mica</th>
                            <th>Keypoints</th>
                            <th>Matches</th>
                            <th>Inliers</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="background: #d1fae5;">
                            <td><strong>SIFT</strong></td>
                            <td><strong>34.26</strong></td>
                            <td><strong>9395√ó9612</strong></td>
                            <td><strong>169,810</strong></td>
                            <td><strong>11,926</strong></td>
                            <td><strong>6,537</strong></td>
                        </tr>
                        <tr>
                            <td>ORB</td>
                            <td>11.75</td>
                            <td>9227√ó9709</td>
                            <td>8,000</td>
                            <td>899</td>
                            <td>592</td>
                        </tr>
                        <tr>
                            <td>AKAZE</td>
                            <td>19.47</td>
                            <td>10251√ó9379</td>
                            <td>73,528</td>
                            <td>4,173</td>
                            <td>2,250</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 4: Comparaci√≥n de detectores en la creaci√≥n de panor√°micas. SIFT ofrece la mejor 
                calidad aunque con mayor tiempo de ejecuci√≥n.
            </p>

            <div class="image-container">
                <img src="results/panoramic/comparacion_metricas_detectores_pyramid.jpg" alt="Comparaci√≥n de m√©tricas">
                <p class="image-caption">
                    Figura 7: Comparaci√≥n cuantitativa de m√©tricas entre detectores para la creaci√≥n de panor√°micas.
                </p>
            </div>

            <h3>4.5 Comparaci√≥n de T√©cnicas de Blending</h3>
            
            <p>
                Comparamos dos t√©cnicas de blending: Feather blending y Laplacian Pyramid Blending.
            </p>

            <div class="image-grid">
                <div class="image-grid-item">
                    <img src="results/panoramic/pair_SIFT_feather.jpg" alt="Feather blending">
                    <h4>Feather Blending</h4>
                    <p>Transiciones suaves pero con halos visibles en zonas de diferente exposici√≥n</p>
                </div>
                <div class="image-grid-item">
                    <img src="results/panoramic/pair_SIFT_laplacian.jpg" alt="Laplacian pyramid blending">
                    <h4>Laplacian Pyramid</h4>
                    <p>Mejor manejo de diferencias de iluminaci√≥n y preservaci√≥n de detalles finos</p>
                </div>
            </div>

            <p class="image-caption">
                Figura 8: Comparaci√≥n de t√©cnicas de blending. Laplacian Pyramid Blending produce 
                resultados visualmente superiores con transiciones m√°s naturales.
            </p>

            <div class="image-container">
                <img src="results/panoramic/trio_SIFT_feather.jpg" alt="Panor√°mica final con tres im√°genes">
                <p class="image-caption">
                    Figura 9: Panor√°mica final fusionada de las tres im√°genes del comedor usando SIFT 
                    y Laplacian Pyramid Blending.
                </p>
            </div>

            <h3>4.6 Calibraci√≥n y Mediciones</h3>
            
            <p>
                Se realiz√≥ calibraci√≥n usando un objeto de referencia conocido (cuadro de 117 cm) y se aplicaron 
                mediciones sobre la panor√°mica generada.
            </p>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Tipo</th>
                            <th>Objeto</th>
                            <th>Longitud Medida (px)</th>
                            <th>Longitud Medida (cm)</th>
                            <th>Longitud Real (cm)</th>
                            <th>Error Relativo (%)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="background: #e0f2fe;">
                            <td><strong>Referencia</strong></td>
                            <td><strong>Cuadro</strong></td>
                            <td><strong>2129.69</strong></td>
                            <td><strong>117.0</strong></td>
                            <td><strong>117.0</strong></td>
                            <td><strong>0.0</strong></td>
                        </tr>
                        <tr>
                            <td>Validaci√≥n</td>
                            <td>Mesa</td>
                            <td>907.85</td>
                            <td>49.87</td>
                            <td>161.1</td>
                            <td>69.04</td>
                        </tr>
                        <tr>
                            <td>Medici√≥n</td>
                            <td>Objeto 1</td>
                            <td>565.41</td>
                            <td>31.06</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p class="image-caption">
                Tabla 5: Resultados de calibraci√≥n y mediciones sobre la panor√°mica. La escala calculada 
                fue de 0.055 cm/px. El error en la validaci√≥n con la mesa indica limitaciones del modelo 
                de homograf√≠a ante paralaje en escenas no planas.
            </p>

            <div class="image-container">
                <img src="results/figures/punto_3/mediciones_visualizacion.jpg" alt="Visualizaci√≥n de mediciones">
                <p class="image-caption">
                    Figura 10: Visualizaci√≥n de todas las mediciones realizadas sobre la panor√°mica generada.
                </p>
            </div>

            <!-- An√°lisis y Discusi√≥n -->
            <h2>5. An√°lisis y Discusi√≥n</h2>

            <h3>5.1 Comparaci√≥n de Diferentes M√©todos Probados</h3>
            
            <h4>Detectores de Caracter√≠sticas</h4>
            <p>
                La comparaci√≥n sistem√°tica de SIFT, ORB y AKAZE revel√≥ diferencias significativas:
            </p>
            <ul>
                <li><strong>SIFT:</strong> Ofrece la mayor precisi√≥n (RMSE = 0.361 px) y robustez, aunque con mayor 
                costo computacional. Ideal para aplicaciones donde la precisi√≥n es cr√≠tica.</li>
                <li><strong>ORB:</strong> Significativamente m√°s r√°pido pero con errores mucho mayores (RMSE = 3.366 px). 
                No recomendado para aplicaciones que requieren alta precisi√≥n, aunque puede ser √∫til para aplicaciones 
                en tiempo real con transformaciones muy peque√±as.</li>
                <li><strong>AKAZE:</strong> Buen compromiso entre velocidad y precisi√≥n. Mejor error angular (0.009¬∞) 
                pero mayor error de traslaci√≥n que SIFT. Alternativa vers√°til.</li>
            </ul>

            <h4>T√©cnicas de Blending</h4>
            <p>
                La comparaci√≥n entre Feather blending y Laplacian Pyramid Blending mostr√≥ que:
            </p>
            <ul>
                <li><strong>Feather Blending:</strong> M√°s r√°pido computacionalmente pero produce halos visibles en 
                zonas con diferencias de exposici√≥n.</li>
                <li><strong>Laplacian Pyramid Blending:</strong> M√°s costoso pero produce transiciones m√°s naturales 
                y preserva mejor los detalles finos. Recomendado para panor√°micas de alta calidad.</li>
            </ul>

            <h3>5.2 An√°lisis de Errores y Limitaciones</h3>
            
            <h4>Errores en Im√°genes Sint√©ticas</h4>
            <p>
                Los resultados con im√°genes sint√©ticas mostraron excelente precisi√≥n:
            </p>
            <ul>
                <li><strong>RMSE promedio:</strong> 0.498 ¬± 0.312 px (precisi√≥n sub-p√≠xel)</li>
                <li><strong>Error angular promedio:</strong> 0.061 ¬± 0.072¬∞ (muy bajo)</li>
                <li><strong>Error de escala promedio:</strong> 0.244 ¬± 0.304% (excelente)</li>
            </ul>
            <p>
                Sin embargo, se observ√≥ que transformaciones m√°s extremas (>25¬∞ rotaci√≥n) pueden aumentar ligeramente 
                los errores, aunque se mantienen dentro de rangos aceptables.
            </p>

            <h4>Limitaciones del Modelo de Homograf√≠a</h4>
            <p>
                En la aplicaci√≥n a im√°genes reales, identificamos varias limitaciones:
            </p>
            <ul>
                <li><strong>Paralaje:</strong> El modelo de homograf√≠a asume una escena plana. En escenas con m√∫ltiples 
                planos de profundidad (como el comedor con objetos cercanos y fondo), se observan duplicaciones y 
                desplazamientos en objetos cercanos.</li>
                <li><strong>Variaciones de Iluminaci√≥n:</strong> Diferencias de exposici√≥n entre im√°genes pueden producir 
                transiciones visibles, aunque el Laplacian Pyramid Blending las mitiga significativamente.</li>
                <li><strong>Campo Visual:</strong> Im√°genes con √°ngulos muy diferentes requieren m√°s interpolaci√≥n en 
                los bordes, aumentando √°reas negras del warping.</li>
            </ul>

            <h4>Errores en Mediciones</h4>
            <p>
                El error del 69% en la validaci√≥n con la mesa (medida estimada: 49.87 cm vs real: 161.1 cm) indica 
                que el modelo de homograf√≠a no puede corregir completamente el paralaje. Esto es esperado ya que la 
                mesa est√° en un plano diferente al cuadro de referencia usado para calibraci√≥n.
            </p>

            <h3>5.3 Posibles Mejoras</h3>
            
            <p>
                Basado en el an√°lisis de resultados, proponemos las siguientes mejoras:
            </p>

            <h4>Mejoras T√©cnicas</h4>
            <ul>
                <li><strong>Bundle Adjustment:</strong> Implementar optimizaci√≥n global de todas las homograf√≠as 
                simult√°neamente para reducir errores acumulativos en panor√°micas con m√∫ltiples im√°genes.</li>
                <li><strong>Seam Finding:</strong> Usar algoritmos de graph-cut o dynamic programming para encontrar 
                costuras √≥ptimas en zonas de transici√≥n, eliminando duplicaciones visibles.</li>
                <li><strong>Equalizaci√≥n de Histograma:</strong> Aplicar normalizaci√≥n de histograma por canal antes 
                del registro para uniformar la iluminaci√≥n entre im√°genes.</li>
                <li><strong>Registro Jer√°rquico:</strong> Para transformaciones muy grandes, aplicar registro en m√∫ltiples 
                niveles de escala para mejorar la robustez.</li>
            </ul>

            <h4>Mejoras en Captura</h4>
            <ul>
                <li><strong>Rotaci√≥n alrededor del centro √≥ptico:</strong> Capturar im√°genes rotando solo alrededor 
                del centro √≥ptico de la c√°mara para minimizar paralaje.</li>
                <li><strong>Mayor solape:</strong> Aumentar el solape entre im√°genes consecutivas (60-70%) para mejorar 
                la robustez del matching.</li>
                <li><strong>Exposici√≥n consistente:</strong> Usar modo manual de exposici√≥n para mantener iluminaci√≥n 
                consistente entre capturas.</li>
            </ul>

            <h4>Mejoras en Calibraci√≥n</h4>
            <ul>
                <li><strong>M√∫ltiples objetos de referencia:</strong> Usar varios objetos de referencia en diferentes 
                planos para calibrar mejor el paralaje.</li>
                <li><strong>Modelos 3D:</strong> Para mediciones precisas en escenas no planas, considerar modelos 3D 
                o t√©cnicas de estructura desde movimiento (SfM).</li>
            </ul>

            <!-- Conclusiones -->
            <h2>6. Conclusiones</h2>
            
            <p>
                Este proyecto ha demostrado la efectividad de un pipeline de registro de im√°genes basado en detecci√≥n 
                de caracter√≠sticas para crear panor√°micas de alta calidad. Las principales conclusiones son:
            </p>

            <ol>
                <li>
                    <strong>Validaci√≥n exitosa:</strong> El pipeline demostr√≥ excelente precisi√≥n en im√°genes sint√©ticas 
                    con RMSE promedio de 0.498 px y errores angulares menores a 0.1¬∞, validando la correcta implementaci√≥n 
                    del sistema.
                </li>
                <li>
                    <strong>SIFT como mejor detector:</strong> Entre los tres detectores evaluados, SIFT ofreci√≥ la mayor 
                    precisi√≥n (RMSE = 0.361 px) y robustez, aunque con mayor costo computacional. ORB result√≥ inadecuado 
                    para aplicaciones que requieren alta precisi√≥n debido a su RMSE significativamente mayor (3.366 px).
                </li>
                <li>
                    <strong>Ratio test √≥ptimo:</strong> El valor tradicional de 0.75 mantiene un balance apropiado 
                    entre cantidad de correspondencias y calidad, aunque valores entre 0.6-0.75 ofrecen resultados 
                    similares.
                </li>
                <li>
                    <strong>Laplacian Pyramid Blending superior:</strong> El blending mediante pir√°mide laplaciana produjo 
                    resultados visualmente superiores al feather blending, eliminando halos y preservando detalles finos.
                </li>
                <li>
                    <strong>Limitaciones del modelo:</strong> El modelo de homograf√≠a tiene limitaciones ante paralaje en 
                    escenas no planas, como se evidenci√≥ en los errores de medici√≥n. Esto es esperado y requiere t√©cnicas 
                    m√°s avanzadas para correcci√≥n completa.
                </li>
                <li>
                    <strong>Aplicabilidad pr√°ctica:</strong> El pipeline implementado es adecuado para crear panor√°micas 
                    de alta calidad en escenas con caracter√≠sticas distintivas y solape adecuado entre im√°genes.
                </li>
            </ol>

            <p>
                El proyecto demuestra que la validaci√≥n cuantitativa con im√°genes sint√©ticas es esencial antes de aplicar 
                algoritmos a im√°genes reales, permitiendo entender limitaciones y optimizar par√°metros de manera objetiva.
            </p>

            <!-- Referencias -->
            <h2>7. Referencias</h2>
            
            <ol>
                <li>
                    Lowe, D. G. (2004). <em>Distinctive image features from scale-invariant keypoints</em>. 
                    International Journal of Computer Vision, 60(2), 91-110.
                </li>
                <li>
                    Rublee, E., Rabaud, V., Konolige, K., & Bradski, G. (2011). <em>ORB: An efficient 
                    alternative to SIFT or SURF</em>. IEEE International Conference on Computer Vision (ICCV).
                </li>
                <li>
                    Alcantarilla, P. F., Nuevo, J., & Bartoli, A. (2013). <em>Fast explicit diffusion for 
                    accelerated features in nonlinear scale spaces</em>. British Machine Vision Conference (BMVC).
                </li>
                <li>
                    Fischler, M. A., & Bolles, R. C. (1981). <em>Random sample consensus: a paradigm for 
                    model fitting with applications to image analysis and automated cartography</em>. 
                    Communications of the ACM, 24(6), 381-395.
                </li>
                <li>
                    Burt, P. J., & Adelson, E. H. (1983). <em>A multiresolution spline with application to 
                    image mosaics</em>. ACM Transactions on Graphics, 2(4), 217-236.
                </li>
                <li>
                    Szeliski, R. (2022). <em>Computer Vision: Algorithms and Applications</em> (2nd ed.). 
                    Springer. Chapter 6: Feature Detection and Matching.
                </li>
                <li>
                    Hartley, R., & Zisserman, A. (2004). <em>Multiple View Geometry in Computer Vision</em> 
                    (2nd ed.). Cambridge University Press. Chapter 4: Estimation - 2D Projective Transformations.
                </li>
                <li>
                    Brown, M., & Lowe, D. G. (2007). <em>Automatic panoramic image stitching using invariant 
                    features</em>. International Journal of Computer Vision, 74(1), 59-73.
                </li>
            </ol>

            <!-- An√°lisis de Contribuci√≥n Individual -->
            <h2>8. An√°lisis de Contribuci√≥n Individual</h2>
            
            <p>
                Este proyecto fue desarrollado en equipo. A continuaci√≥n se detalla la contribuci√≥n de cada integrante:
            </p>

            <h3>8.1 Parte 1: Validaci√≥n con Im√°genes Sint√©ticas</h3>
            <ul>
                <li><strong>Carlos Andr√©s Viera Mosquera</strong> (<a href="mailto:cviera@unal.edu.co">cviera@unal.edu.co</a>)</li>
            </ul>
            <p>
                Desarrollo completo de la validaci√≥n con im√°genes sint√©ticas, incluyendo:
            </p>
            <ul>
                <li>Implementaci√≥n de funciones para crear im√°genes sint√©ticas con transformaciones conocidas</li>
                <li>Desarrollo de las clases FeatureDetector, FeatureMatcher y RegistrationEvaluator</li>
                <li>Ejecuci√≥n de experimentos sobre 15 im√°genes sint√©ticas con diferentes transformaciones</li>
                <li>Comparaci√≥n sistem√°tica de detectores (SIFT, ORB, AKAZE) con an√°lisis cuantitativo</li>
                <li>Estudio del impacto del ratio test en el rango [0.5, 0.95]</li>
                <li>Desarrollo de m√©tricas de error (RMSE, error angular, error de traslaci√≥n, error de escala)</li>
                <li>Creaci√≥n del notebook 01_registro_imagenes_proyecto.ipynb con documentaci√≥n y visualizaciones</li>
            </ul>

            <h3>8.2 Parte 2: Registro de las Im√°genes del Comedor</h3>
            <ul>
                <li><strong>Carlos Andr√©s Viera</strong>(<a href="mailto:cviera@unal.edu.co">cviera@unal.edu.co</a>)</li>
                <li><strong>Yenifer Tatiana Guavita Ospino</strong> (<a href="mailto:yguavita@unal.edu.co">yguavita@unal.edu.co</a>)</li>
            </ul>
            <p>
                Desarrollo conjunto del registro de im√°genes reales y creaci√≥n de panor√°micas:
            </p>
            <ul>
                <li>Implementaci√≥n de la clase Stitcher con detecci√≥n multi-escala y blending mediante pir√°mide laplaciana</li>
                <li>Aplicaci√≥n a im√°genes reales: creaci√≥n de panor√°micas a partir de tres im√°genes del comedor</li>
                <li>Comparaci√≥n de t√©cnicas de blending (feather, laplacian pyramid)</li>
                <li>Evaluaci√≥n de diferentes detectores en el contexto de panor√°micas</li>
                <li>Creaci√≥n del notebook 02_registro_imagenes_comedor.ipynb con an√°lisis y resultados</li>
            </ul>

            <h3>8.3 Parte 3: Calibraci√≥n y Medici√≥n</h3>
            <ul>
                <li><strong>Lina Mar√≠a Montoya Zuluaga</strong> (<a href="mailto:limontoyaz@unal.edu.co">limontoyaz@unal.edu.co</a>)</li>
                <li><strong>Yojan Tamayo Montoya</strong> (<a href="mailto:ytamayom@unal.edu.co">ytamayom@unal.edu.co</a>)</li>
            </ul>
            <p>
                Desarrollo conjunto del sistema de calibraci√≥n y mediciones:
            </p>
            <ul>
                <li>Implementaci√≥n del sistema de calibraci√≥n usando objetos de referencia</li>
                <li>Desarrollo de funciones para medici√≥n de objetos sobre la panor√°mica generada</li>
                <li>Creaci√≥n del script interactivo punto3.py para mediciones</li>
                <li>Validaci√≥n de la calibraci√≥n con objetos conocidos</li>
                <li>Creaci√≥n del notebook 03_registro_imagenes_calibracion_y_medicion.ipynb</li>
                <li>Generaci√≥n de visualizaciones con mediciones marcadas</li>
            </ul>

            <h3>8.4 Documentaci√≥n y Reporte</h3>
            <p>
                La documentaci√≥n del proyecto, incluyendo este reporte t√©cnico, README.md y visualizaciones, fue desarrollada 
                colaborativamente por todo el equipo.
            </p>

            <div class="info-box">
                <h4>üìÇ Repositorio del Proyecto</h4>
                <p>
                    El c√≥digo completo de este proyecto, incluyendo todos los scripts de an√°lisis, visualizaci√≥n y 
                    notebooks interactivos, est√° disponible en GitHub. El repositorio incluye documentaci√≥n detallada 
                    para reproducir todos los experimentos presentados en este reporte.
                </p>
            </div>

        </article>
    </div>
</body>
</html>
